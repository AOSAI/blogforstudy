---
title: 2-2 神经网络进阶
order: 6
author: AOSAI
date: 2024-02-03
category:
  - 机器学习
tag:
  - 神经网络
  - 深度学习
  - TensorFlow
---

神经网络进阶（Deep Learning）

## 1.TensorFlow 代码实现推理

TensorFlow 是实施深度学习算法的领先框架之一，另一个热门框架是 PyTorch。但在这里我们仅用前者做讲解（因为吴教授用这个比较多）。

### 1.1 咖啡烘培案例

有些喜欢喝咖啡的朋友，可能会专门的买咖啡豆回来自己烘培，那么能否用学习算法来优化烘焙过程，从而获得烘培质量的提升呢？

![6.1 咖啡烘培实验](/machinelearning/two/06-7.png =560x)

烘培咖啡简化的来说，你需要控制的两个参数分别是：烘培的温度，以及烘培的时长。这张示例图中，横坐标表示温度，纵坐标表示时长，中间的红叉表示 1，对应好咖啡，蓝色的圈表示 0，对应坏咖啡。

如果烘培的温度太低，又或者时间太短，可能烘培不熟，导致咖啡豆的质量不够好。如果温度太高，或者时间过长，可能咖啡豆就焦了，也会导致口感变差。只有在三角形区域内的点对应好咖啡。

假设我们想知道，在温度 200 华氏度，时长 17 分中的情况下，烘焙出的咖啡豆的质量是好是坏，基于已有的知识，我们可以大概的写出这样的代码。

![6.2 咖啡烘培代码初见](/machinelearning/two/06-8.png =560x)

这里用 numpy 构建了一个二维数组，Dense 只是一个名字，表示神经网络层，里面的激活方式 activation 选择了 sigmoid，这是逻辑回归，units 表示有 3 个神经元。a1 表示第一个隐藏层输出的三元向量。 a2 就表示预测结果，是好咖啡豆的概率。最后对 a2 做一个条件判断，就得出了我们要的结果。

同理我们可以回推手写数字识别的伪代码实现过程。

![6.3 手写数字识别代码初见](/machinelearning/two/06-9.png =560x)

### 1.2 TensorFlow 的数据形式

盲生，我发现了华点！Tensorflow 竟然是由吴恩达教授发起并领导过的谷歌团队制作的，太厉害了。但遗憾的是，NumPy 是很多年前就被创建并成为线性代数和 Python 的标准库的，他们俩之间数据表示方式存在不一致的问题。因此，了解 Tensorflow 的一些约定俗成是一件十分必要的事情，这样我们才能敲出正确的代码，并在神经网络模型中成功运行。

![6.4 数据形式的一些微小差异](/machinelearning/two/06-10.png =560x)

如果还记线性回归和逻辑回归的内容，应该清楚当时在 Numpy 里使用的都是一维数组，也就是向量的形式去处理的数据，而 Tensorflow 的惯例是使用矩阵来表示数据，因为它的目标是在于处理非常大的数据集，通过矩阵的计算，可以让它内部的计算效率更高。因此呢，我们在 Numpy 中，写成了二维数组的形式，来代表它是矩阵。

a1 表示第一个隐藏层的激活，是一个行向量，也就是一行三列的二维数组。如果你把 a1 打印出来，你会得到 tf.Tensor(...)这样一个东西。shape=（1，3）表示这是一个 1 行 3 列的矩阵，这个和 Numpy 是一致的，如果不明白请看 Numpy 文档。dtype 就是数据格式。

这里的 Tensor（张量）是 Tensorflow 团队为了有效的存储和执行矩阵计算而创建的一种数据类型。当然了，如果你想将格式转换回 NumPy 数组，你可以使用 a1.numpy()方法来实现。

### 1.3 初次搭建神经网络

![6.5 咖啡烘培神经网络初搭建](/machinelearning/two/06-11.png =560x)

这就是 TensorFlow 搭建一个神经网络的大致过程，首先，我们通过 Sequential 函数顺序的将神经网络层排列起来；然后，通过 compile 函数将其编译；最后通过 fit 函数将其实现。数据集就是图中显示的 x 和 y 的数组。

可以看的出来，它的代码数量很少，非常的简洁。通常我们使用已有的神经网络模型，可以直接调用最顶尖的库，也许五六行代码就可以实现我们的目标。但是学习原理会有助于我们更好的使用这些库，以及去解决代码实现过程中的一些问题。因此，我们来看看如果仅仅是使用 Numpy，如何完成咖啡烘培的，神经网络模型的代码实现。

### 1.4 Numpy 实现向前传播算法

![6.6 Numpy实现神经网络1](/machinelearning/two/06-12.png =560x)

还是咖啡烘培案例，我们把所有神经元的操作，全部显示的写出来。输入的 x 为 200 的温度，和 17 的时长。绿色的 ① 就是第一层神经元的计算过程，得出的三个值 a1_1，a1_2，a1_3 组成一个新的向量 a1，作为第二层的输入。绿色的 ② 就是第二层，运算的步骤和 ① 是一样的。

这里要注意的是，其余的代码都是真实代码，但是 sigmoid 函数只是一个代指，如果要使用，请参考 Numpy 函数手册，或者自行实现。如果说逻辑回归的计算过程忘记了，不要着急，等会第 4 节马上会复习。

![6.7 Numpy实现神经网络2](/machinelearning/two/06-13.png =560x)

图 6.6 只是为了让大家清晰的明白在每一个神经元的运算过程，在真实的代码实现里，我们一定会使用循环语句去代替这些重复的操作。看看图 6.7，我们用类似 TersorFlow 的格式，来实现向前传播算法。

先看 dense 函数，W 的 shape 是（2，3），表示两行三列，它取了索引 1，也就是第二个值 3。然后创建了一个新的返回值 a_out = [0, 0, 0]，然后通过循环，计算出了不同的激活值，分别放在了 a_out[0]、a_out[1] 中，依次类推，最后返回的 a_out 就是一个神经网络层中的输出，也就是激活，在 sequential 函数中传递给下一层。

这里要注意的是：==有几层神经网络，sequential 函数中就有几个 dense。== 图中的代码仅仅是举例说明，如果要带入咖啡烘焙的案例，那么 sequential 函数中的 dense 只有两个，因为只有两层。

## 2.神经网络为何如此高效

### 2.1 Numpy 和 TensorFlow 的对比

科学家们之所以能够扩展中型甚至是大型神经网络模型的原因之一，是因为神经网络可以向量化。它可以使用矩阵乘法，能非常有效的实现计算。

事实证明，GPU 在内的并行计算硬件，也包括一些 CPU，它们非常善于做非常大的矩阵乘法。下面就来看看神经网络的矢量化，是如何实现工作的。如果没有这些想法，我觉得深度学习在今天不会取得这样的成功和规模。

![6.8 Numpy 和 TensorFlow 的对比](/machinelearning/two/06-14.png =560x)

图片的左边，是如何在单层中实现向前传播算法的原始代码。我们可以通过右边这样的方式进行优化，将 X 和 B 全部写成二维数组，也就是矩阵的形式，然后在 dense 函数中，就可以通过两行代码，去代替之前的循环计算，大大的提升了计算的效率。np.matmul() 函数的作用是做矩阵乘法，而 g(Z)表示激活函数，也就是逻辑回归。

### 2.2 矩阵乘法原理

在 Numpy 中我有浅浅的写过 dot 函数的作用。dot()函数称为点积，也叫内积。向量内积或者说矩阵内积，必须要满足的 **前提条件** 就是，==前一个向量的列数要与后一个向量的行数要对应==，否则没有办法相乘。简单的写一下行向量和列向量相乘：

$$
\begin{bmatrix} 1 & 2 & 3  \end{bmatrix} \cdot
\begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix} = 4 + 10 + 18 = 32
$$

我们再看一下反过来，如果是列向量乘以行向量会是什么结果：

$$
\begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix} \cdot
\begin{bmatrix} 1 & 2 & 3  \end{bmatrix} =
\begin{bmatrix} 4 & 8 & 12 \\ 5 & 10 & 15 \\ 6 & 12 & 18 \end{bmatrix}
$$

==点积的相乘规律就是前行乘后列==。因此，行向量点乘列向量，前行乘后列，就变成了数字。列向量点乘行向量，前行乘后列，就变成了一个矩阵。

矩阵可以看作是有行有列的二维数组，也可以看作是多个行向量或者列向量组合而来的数据。==矩阵的相乘前提以及相乘规则，和向量是一致的。== 我们来计算一个 2×2 的矩阵相乘：

$$
\begin{bmatrix} 1 & 2  \\ 3 & 4 \end{bmatrix} \cdot
\begin{bmatrix} 5 & 6  \\ 7 & 8  \end{bmatrix} =
\begin{bmatrix} 1*5+2*7 & 1*6+2*8 \\ 3*5+4*7 & 3*6+4*8 \end{bmatrix} =
\begin{bmatrix} 19 & 22  \\ 43 & 50  \end{bmatrix}
$$

## 3.TensorFlow 训练神经网络

### 3.1 回顾逻辑回归的模型训练

1. 第一步，是指定如何在给定输入 x 以及参数 w 和 b 的情况下计算输出。

```
z = np.dot(w, x) + b
f_x = 1 / (1 + np.exp(-z))
```

2. 第二步，指定损失函数（成本函数）。损失函数 L 是衡量逻辑回归在单个训练示例（x，y）上表现如何的指标。而成本函数 J 是在整个训练集上计算的损失函数的平均值。

$$
L(f_{\vec{w},b}(\vec{x}),y) = loss \\
J(\vec{w},b) = \frac{1}{m}\sum_{i=1}^{m}L(f_{\vec{w},b}(\vec{x}),y)
$$

```
loss = -y * np.log(f_x) - (1-y) * np.log(1 - f_x)
```

在统计学中，这个 loss 函数诶被称作为交叉熵损失函数（BinaryCrossentropt()），二进制（binary）这个词只是为了强调这是一个二元分类问题，要么 0 要么 1。

3. 最后一步，是使用一种特定的梯度下降算法，来最小化逻辑回归的成本函数 J。其中 w 更新为 w 减去学习率 alpha 乘以成本函数 J 相对于 w 的导数。b 同样的做法。

```
w = w - alpha * dj_dw
b = b - alpha * dj_db
```

### 3.2 训练神经网络的完整步骤

![6.9 TensorFlow 训练模型完整过程](/machinelearning/two/06-15.png =560x)

以手写数字识别为例：

1. 第一步，指定模型，告诉 TensorFlow 如何计算推理。Sequential()函数表示将神经网络的这三层，顺序的串在一起。 Dense()表示不同的神经网络层，units 表示神经元数数量，activation 表示神经元的激活方式，在这里 sigmoid 就表示用逻辑回归的方式激活。

2. 第二步，使用特定的损失函数让 TensorFlow 编译模型。compile()函数，loss 表示你需要指定的损失函数是什么。在这个案例里，我们使用二元交叉熵损失函数。如果你将这个损失在整个训练集中取了平均，你就得到了神经网络的成本函数。

3. 第三步，训练模型，调用 fit()函数使得神经网络的成本函数最小化。调用步骤 ② 中的损失函数来拟合你在步骤 ① 中指定的模型到数据集 X，Y。epochs 表示梯度下降的次数。

<font color="blue">一个炒冷饭知识，Keras 原本是一个和 TensorFlow 完全不一样的库，但是最后被 TensorFlow 合并了，我们的交叉熵损失函数就是从它这里导入的包。如果你想解决的是回归问题，而不是分类问题，你可以选择不同的损失函数来编译你的模型，就比如 Keras 中的 MeanSquareError() 函数，很熟悉对吧，没错就是平方误差损失函数。</font>

### 3.3 激活函数的替换选项

到目前为止，我们一直在隐藏层和输出层的所有节点中使用 Sigmoid 激活函数。之所以如此，是因为从 T 恤预测案例开始，我们通过采用逻辑回归，创建大量逻辑回归单元，并把它们串在一起来构建向前传播的神经网络。但如果你适当的改变激活函数，你的神经网络会变得更加强大。

回想一下上一章的 T 恤复杂预测案例，在给定价格、运费、营销、材料的情况下，你尝试预测 T 恤是否畅销。我们对可能畅销的 T 恤做了如下的假设：具有良好的知名度和高感知的质量。

但是这个假设是二元的，非黑即白，人们要么知道，要么不知道。实际上潜在买家对于一个物品的了解程度分为了很多个层次，比如：不知道、知道一点、有些了解、非常了解、口口相传。因此，与其将 awareness 建模为二进制的 0 和 1（sigmoid 函数），不如尝试估计 awareness 的概率（ReLU 函数）。

![6.10 最常见的三个激活函数](/machinelearning/two/06-16.png =560x)

ReLU（Rectified Linear Unit）函数是指，如果 z 小于零，那么 g(z) = 0；如果 z 大于零，那么 g(z) = max(0, z) = z。除了 ReLU 函数，还有一个最常见的激活函数，那就是线性激活函数。

线性激活函数由图可知，g(z) = z，而 a=g(z)，因此 a=z，等于 g(z)这个激活函数什么也没有做。所以有的人会说：**我们没有使用任何的激活函数。<-- 这句话就代表用的是线性激活函数。**

线性激活函数、sigmoid 激活函数、ReLU 激活函数，这三个是最常见也是用的最多的激活函数。在后面的内容里，会讲一个叫做 softmax 的激活函数。

### 3.4 如何选择激活函数

事实证明，根据真实标签 y，输出层的激活函数会有一个相当自然的选择。

- 比如，我们正在处理二元分类问题，y 的值只有 0 和 1，那么输出层就选择 sigmoid 函数。
- 如果我们要解决回归问题，比如我们试图预测明天的股票价格与今天的股票价格相比会如何变化，它可能上涨，也可能下跌，y 的范围在[-∞，+∞]，那么就选择线性激活函数。
- 最后，如果 y 只能取正值，例如你要预测房屋价格，它永远不可能是负值，输出层就理所当然的选择 ReLU 函数。

除了输出层，我们也看看用于隐藏层的激活函数的选择。

![6.11 隐藏层的激活函数选择](/machinelearning/two/06-17.png =560x)

同样由数据表明，ReLU 激活函数是迄今为止，众多从业者训练神经网络的最常见选择。sigmoid 激活函数在早期的神经网络世界里，是用的最多的，但是如今已经很少被用了，被 ReLU 所替代。

我们对比一下两个激活函数：**首先**，ReLU 的计算速度更快一点，因为它只需要计算 0 和最大值 z；而 sigmoid 需要先取幂，再取反等等，所以它的效率会低一点。**其次**，ReLU 函数仅在图形的一部分变平坦（flat），而 sigmoid 函数在两个部分变平坦。如果梯度下降时存在很多很胖（fit）的地方，效率就会大大降低。

梯度下降虽然是优化的 W、B 的成本函数 J，而不是优化的激活函数，但是激活函数也是整个计算的一部分，这就会导致成本函数 J 中的许多地方也是平坦的，并且梯度很小，这就减慢了学习速度。如图 6.11 中间的蓝色坐标轴。

这样子一对比，研究人员就发现，使用 ReLU 激活函数可以使你的神经网络学习的更快一点，这就是为什么它成为目前最常见的选择的原因。

因此，激活函数选择的建议就是，输出层根据 y 的值去决定激活函数类型，而隐藏层建议使用 ReLU 激活函数作为默认的隐藏层激活函数。有了这组更丰富的激活函数，你可以构建出比仅使用 sigmoid 激活函数更好更强大的神经网络。还是以手写数字识别为案例：

```py
from tf.keras.layers import Dense
model = Sequential([
    Dense(units=25,activation="relu"),
    Dense(units=15,activation="relu"),
    Dense(units=1,activation="sigmoid")
])
# 线性激活函数叫做 "linear"
```

顺便说一句，如果你查看研究文献，有时你会看到作者使用其他的激活函数，例如 tan h 激活函数，或 LeakyReLU 激活函数，或 swish 激活函数等等。 可能有的时候它的效果比 ReLU 更好一点，但是在绝大多数情况下，对于绝大多数应用，这三个基本的激活函数就已经足够好了。

当然，如果你想要了解更多其他的激活函数的信息，请自行上网查阅资料。这种其它类型的激活函数，可以做的更加强大的情况只是少数。

### 3.5 为什么需要激活函数

我们使用神经网络模型，是为了拟合比线性回归、比逻辑回归更加复杂的结构。如果我们把所有的激活函数都换成线性回归，如下图：

![6.12 为什么需要激活函数](/machinelearning/two/06-18.png =560x)

我们做一个简单的推导，当隐藏层和输出层都为线性激活函数时，我们发现它最终的结果还是一个线性回归函数。既然这样，直接用线性回归就好了，对吧，方便还省事儿。

如果我们把输出层换成逻辑激活函数呢，逻辑回归 g(z) 中的 z 不还是线性回归嘛，这样子就变成了一个逻辑回归任务，哪怕你模型再大，神经网络层再多，神经元再多，还是单纯做的是逻辑回归，那么使用神经网络就没有意义了。

所以在我看来，激活函数的存在一定程度上是为了区分复杂度，不是说在所有的应用领域里神经网络都是最好的，能用线性回归解决的就用线性回归，能用逻辑回归解决的就用逻辑回归，合适的才是最好的。

## 4.多类分类问题（Multiclass）

多类分类问题本质上还是分类问题，只是你可以有两个以上的输出标签，而不仅仅只是 0 或 1。

例如手写数字识别案例，我们只是区分了 0 和 1。如果你想尝试阅读信封中的协议或者邮政编码，那么实际上你可能需要识别 1 到 9 的十个数字。

或者是工厂零件制造商的视觉缺陷检查，也许你会看一家制药公司的药丸图片，并试图弄清它是否有划痕效应、变色缺陷或芯片缺陷。这又是多种不同类型的缺陷，通过这些可以给药丸进行分类。

![6.13 多类分类问题](/machinelearning/two/06-19.png =560x)

在分类问题的最开始，我们讲了一个肿瘤预测的案例，它是良性的还是恶性的，如左图，它是一个二分类问题，要么是良性、要么就是恶性。

多类分类问题可能会如同右边的图，有多种结果。会有一个叫做决策边界的新知识出现。

### 4.1 softmax 回归算法

逻辑回归适用于 y 可以取两个可能的输出值（0 或 1）的情况。

$$
z = \vec{w}\cdot{\vec{x}} + b \\
a_{1} = g(z) = \frac{1}{1+e^{-z}} = P(y=1|\vec{x}) \\
a_{2} = 1 - a_{1} = P(y=0|\vec{x})
$$

a1 表示逻辑回归估计，在给定这些输入特征 x 的情况下，y=1 的概率。在二分类问题中，可能和不可能的概率加起来是等于 1 的，所以 y=0 的概率，就是 1-（y=1 的概率）。

我们将其推广到多分类问题，如图 6.13 的四种分类：

$$
z_{1} = \vec{w_{1}}\cdot{\vec{x}} + b_{1} \\
a_{1} = \frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} = P(y=1|\vec{x})
$$

$$
z_{2} = \vec{w_{2}}\cdot{\vec{x}} + b_{2} \\
a_{2} = \frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} = P(y=2|\vec{x})
$$

$$
z_{3} = \vec{w_{3}}\cdot{\vec{x}} + b_{3} \\
a_{3} = \frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} = P(y=3|\vec{x})
$$

$$
z_{4} = \vec{w_{4}}\cdot{\vec{x}} + b_{4} \\
a_{4} = \frac{e^{z_4}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} = P(y=4|\vec{x})
$$

由这个例子，我们可以归纳出，Softmax 回归的公式：

$$
z_{j} = \vec{w_{j}}\cdot{\vec{x}} + b_{j} \quad j=1,2...,N
$$

$$
a_{j} = \frac{e^{z_j}}{\sum^{N}_{k=1}e^{z_k}} = P(y=j|\vec{x})
$$

和二分类问题一样，多类分类问题的概率总和还是 1，也就是说 a~1~ + a~2~ + ... + a~N~ = 1 。因此，softmax 回归算法就是逻辑回归算法的推广，使用它你将能够执行多分类问题。我们再来看看它的成本函数：

![6.14 Softmax成本函数](/machinelearning/two/06-20.png =560x)

### 4.2 神经网络的 Softmax

为了构建一个可以进行多分类问题的神经网络，我们将采用 Softmax 回归模型，**本质上是将其放入神经网络的输出层。**

![6.9 TensorFlow 训练模型完整过程](/machinelearning/two/06-15.png =560x)

还是以手写数字识别为例，以防忘记，我直接将图 6.9 搬下来。这时的输出层还是一个单独的逻辑回归，只能识别 0 或 1。我们将其改写成 Softmax 回归，一共有 0~9 十个数字，因此就有十个神经元（具体一点的展开请参考 4.1 小节）：

$$ logistic\_ regression \quad a^{[3]}=g(z^{[3]}) \\ $$

$$
softmax \quad \vec{a}^{[3]}=(a_{1}^{[3]},...,a_{10}^{[3]})=g(z_{1}^{[3]},...,z_{10}^{[3]})
$$

再来看看 Softmax 神经网络的代码实现，和之前的结构几乎没有变化，只是输出层和损失函数做出了调整：

```py
import tensorflow as tf
from tf.keras import Sequential
from tf.keras.layers import Dense
from tf.keras.losses import SparseCategoricalCrossentropy

model = Sequential([
    Dense(units=25,activation="relu"),
    Dense(units=15,activation="relu"),
    Dense(units=10,activation="softmax") # 变化1：输出层
])
model.compile(loss=SparseCategoricalCrossentropy()) # 变化2：损失函数
model.fit(X, Y, epochs = 100)
```

SparseCategoricalCrossentropy 函数：Categorical 是指仍然将 y 分类，softmax 在这里取了 10，所以 y 的结果是从 0 到 9。Sparse 是指稀疏，也就是每次仅能取 0 到 9 中的一个值，比如 3 和 7 两种判断，或者其他多种判断一块出现的结果不会发生。

### 4.3 Softmax 的改进实现

虽然上述代码可以实现，能够正常工作，但还有能改进的空间。计算机在计算的过程中，因为内存是有限的，所以在浮点数（超小数），或者超大数的计算中，会有些许的误差存在：

```py
x1 = 2.0 / 10000
print(f"{x1:.18f}") # 表示输出x1，并且保留小数点后18位
# 0.000200000000000000

x2 = (1 + 1/10000) - (1 - 1/10000)
print(f"{x2:.18f}")
# 0.000199999999999978
```

按道理来讲，x1 和 x2 的结果，应该是一样的才对，但是根据 python 的运算结果发现它们确实存在了细微的误差。还记得逻辑回归的损失函数吗：

$$ loss=-y\log{a}-(1-y)\log{(1-a)} $$

这里的 a 就是逻辑回归的输出，也就是激活，它写做 $a=g(z)=1/(1+e^{-z})$。我们通过线性回归得到了 z（$z=\vec{w}\cdot\vec{x}+b$）,再通过逻辑回归 g(z) 得到了 a，再通过损失函数 loss 计算偏差值。

我们会发现，如果 **存在超大数或者超小数** 每一次的得到具体结果的计算，必定会使得这个结果，存在一点数字上的误差，累积起来，误差就变大了，就如同上述的 Python 计算。

有些小机灵鬼就想到了一个办法，我们把得出 a 值的这个步骤省略掉，直接带入得出 a 值得公式，进入 loss 函数，这样就减小了计算误差：

$$ loss=-y\log(\frac{1}{1+e^{-z}})-(1-y)\log{(1-\frac{1}{1+e^{-z}})} $$

这个方法确实是可行的，不论是 sigmoid 的 loss 函数的两个逻辑回归，还是 softmax 的 loss 函数的多个逻辑回归，它都适用，而且对于 softmax 这种多逻辑回归输出的函数而言，这样优化的效果收益更大。我们来看看代码上，有哪些需要修改的：

::: code-tabs

@tab sigmoid 函数代码修改

```py
model = Sequential([
    Dense(units=25,activation="relu"),
    Dense(units=15,activation="relu"),
    # 逻辑回归中，sigmoid函数换为 linear函数
    Dense(units=1,activation="linear")
])
# 逻辑回归的loss函数中写入：from_logits=True
model.compile(loss=SparseCrossEntropy(from_logits=True))
model.fit(X, Y, epochs = 100)

# 底层逻辑是，原本的逻辑回归输出层中，X应该是a，即逻辑回归函数已经运算完了，
# 而现在X表示的是z，因为 Dense 最后走的是 linear线性回归，
# sigmoid函数的运算直接嵌入到了 loss函数中
logits = model(X)
f_x = tf.nn.sigmoid(logit)
```

@tab softmax 函数代码修改

```py
model = Sequential([
    Dense(units=25,activation="relu"),
    Dense(units=15,activation="relu"),
    # softmax回归中，softmax函数也换为 linear函数
    Dense(units=10,activation="linear")
])
# softmax函数的loss函数中也写入：from_logits=True
model.compile(loss=SparseCategoricalCrossentropy(from_logits=True))
model.fit(X, Y, epochs = 100)
# 同理，softmax中的X也从 a1~a10 变成了 z1~z10
logits = model(X)
f_x = tf.nn.softmax(logit)
```

:::

### 4.4 多标签分类（Multi-label）

有一种不同类型的分类问题，称为多标签分类问题。这种问题，输出的结果是一个向量。

比如你正在创造一辆自动驾驶汽车，或者一个驾驶辅助系统，那么给你一张当下车前的图片，你可能会想知道，你的车前面，有没有汽车？有没有公交车？有没有行人？等等。

![6.15 多标签分类1](/machinelearning/two/06-21.png =560x)

如何去构建这样一个多标签分类的神经网络呢？

思路一：将其视为三个完全独立的机器学习问题。一个检测汽车，一个检测公交车，一个检测行人。但这其实并不合理，工作量繁重。

思路二：训练一个神经网络，同时检测汽车、公交车、行人。

![6.16 多标签分类2](/machinelearning/two/06-22.png =560x)

吴恩达教授在这里也只是提到了这样一种分类形式，并没有说明它要怎么实现，反正必然是和 softmax 不同的，softmax 虽然有多种类别，但仅能选择一种，而多标签分类可以同时存在多个标签。如果有兴趣可以自行上网查阅资料。

## 5.补充内容

### 5.1 Adam 优化算法

梯度下降是一种广泛应用于机器学习的优化算法，是线性回归和逻辑回归等许多算法以及神经网络早期实现的基础。但现在有一些其他的优化算法，最小化成本函数的效果比原始的梯度下降更好。

Adam 算法，因为它可以微小的、自动的调整学习率，所以在梯度下降的速度上、稳健性上都有更好的表现。

![6.17 梯度下降之adam优化算法](/machinelearning/two/06-23.png =560x)

我们知道梯度下降中的学习率，原本是固定值，如果过小，下降速度会很慢，导致运算次数增加，运算成本和时间成本都会增加，而如果过大，就会导致来回跳，甚至永远到达不了最低点。

Adam 算法它可以自动的调整学习率 a 的大小，过大就减小，过小就增大，这就使得梯度下降的稳定性得到了提升。我们不探究怎么实现的，仅看一下在编译 complie 的代码中，应该怎么写：

```py
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=le-3),
      loss=SparseCategoricalCrossentropy(from_logits=True))
```

le-3 表示 10^-3^，也就是 0.001，这是一般情况下 Adam 算法的最常用的初始值。

### 5.2 神经网络层类型

到目前为止，讲过的所有的神经网络层，都是密集层类型，当前层中的每个神经元的输入，都是前一层的激活。除了密集层类型，还有一些其他的网络层以及其他的属性，在这里给大家举个栗子。你可能会在某些工作中看到一种叫做卷积层的神经网络层。

![6.18 梯度下降之adam优化算法](/machinelearning/two/06-24.png =560x)

我在输入层（空白图像），手写一个数字 9，然后构建一个隐藏层，它将计算不同的激活作为图像的输入。就比如，第一个蓝色隐藏单元，它只查看九个方块中的第一个，第二个洋红色隐藏单元，它只查看九个方块中的第二个，依次类推。

可能有朋友就会问了，为什么不让每个神经元，查看所有的像素，而只是查看一块像素呢？

- 优点 1：它加快了计算的速度。
- 优点 2：用这种模型，可以减少训练的数据量。或者说，它也不太容易过度拟合。

这种每个神经元只关注输入图像的一个区域的神经网络层，叫做卷积层。研究人员 John Macoun 弄清楚了让卷积层发挥作用的一些具体的操作，并推广了它们的使用。如果你的神经网络中有多个卷积层，有时它也被称为卷积神经网络。我们再来看一个例子。

![6.19 梯度下降之adam优化算法](/machinelearning/two/06-25.png =560x)

一个心电图的结果 x，我们将其分成 100 份，第一个卷积层，第一个神经元读取 1~20 份图片，第二个神经元读取 11~30 份，第三个 21~40 份，以此类推。一共有 9 个神经元。

第二个卷积层，第一个神经元我们读取 1~5 份，第二个读取 3~7 份，第三个读取 5~9 份。也许在输出层，使用了 sigmoid 函数，对第二个卷积层的激活做了一个是否有心脏病的二进制分类。

对于卷积层，也有很多的架构选择，比如：单个的神经元应该查看的输入窗口有多大；每层有多少个神经元等等。通过这些，我们可以更好的构建卷积神经网络模型。

这一节主要是想告诉大家，网络层是可以有选择的。如果你听说过，Transformer 模型、LSTM 模型、Attention 模型等等，它们的研究人员，其实就是把不同的神经网络层，组合到了一起，形成更复杂的、更强大的神经网络。
