---
title: 2-3 模型评估
order: 7
author: AOSAI
date: 2024-04-23
category:
  - 机器学习
tag:
  - 模型评估
---

<style>
  @media (orientation:landscape){
    .layout{
      display:flex;
    }
  }
  @media (orientation:portrait){
    .layout{}
  }
</style>

模型评估（Model Evaluation）

截止现在，你已经看到了很多机器学习的算法，比如线性回归、逻辑回归、甚至是深度学习（神经网络），你现在手上已经有了很多强大的机器学习的工具，但是该怎么有效的使用这些工具呢？

吴恩达教授说他见过一些团队，用了整整六个月的时间才搭建好一个机器学习的系统，他认为这是应该在几周的时间内完成的事情。

使机器学习系统运行良好的速度或者说效率，很大程度上取决于你在机器学习项目的过程中，反复做出下一步该做什么的正确决策的能力。

让我们从一个老生常谈的例子说起，假设你已经实现了正则化线性函数回归，来预测房价，你的机器学习算法有常见的成本函数（平方误差加上正则化）：

$$
J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m}\sum_{j=1}^{n}w_{j}^{2}
$$

但是，如果你训练模型的过程中，发现它在预测时出现了无法接受的大错误，接下来你会尝试什么？在构建机器学习算法时，我们可以尝试很多种不同的方法：

<font color="#4834d4">

1. 也许会觉得样本太少 --> 获得更多的训练样本

2. 也许觉得特征太多 --> 尝试较少的特征集

3. 或者想要获得额外的功能 --> 比如通过特征工程，创造新的特征

4. 或者尝试对现有的特征 x1、x2 等，创建多项式特征 --> 如 x1 的平方、x2 的平方、x1 乘以 x2

5. 或许你还会怀疑，Lambda 的值是不是选的不好，也许太大了，或者太小了 --> 这时就反向去调整它

</font>

在给定的机器学习的应用程序中，通常你会发现，一些改变是有效果的，一些改变是不会有成果的。有效构建机器学习算法的关键在于，你需要采用一些诊断方法，去知道你真正需要做出调整的地方在哪里。

## 1 通过测试集评估模型性能

还是房价预测的案例，假设我们有 5 组数据，特征仅仅是 size，我们通过 x 的零次方到 x 的四次方，建立一个多项式特征组合，它可以完美的拟合所有的点位，就如下图所示。

![7.1 房价预测模型](/machinelearning/three/07-01.png =560x)

但是很明显它过拟合了，我们认为它无法推广到训练集中的新数据里，所以它并不是一个很好的模型。一个特征的情况下，我们可以通过 matplotlab 绘图来判断模型的好坏，但假如这个模型还有更多的特征呢？

比如图中的 x1 房子大小、x2 卧室数量、x3 楼层数、x4 建筑年龄，在有 4 个特征的情况下，f(x)是一个四维函数，你如何去绘制它？所以我们需要一些更系统的方法，来评估你的模型的表现。

### 1.1 回归模型的评测

我们可以将数据集分为**训练集**和**测试集**两个部分，比例通常是 7:3 或者 8:2。我们要做的是在 70%左右的数据上训练模型、训练参数，在 30%左右的数据上测试它的表现。这种方式可以让你系统的评估你的机器学习成果好不好。

![7.2 训练集和测试集](/machinelearning/three/07-02.png =560x)

在这里我们用下标 train 来表示训练集的数据，mtrain 表示训练集的数据数量，同理下标 test 表示测试集的数据，mtest 表示测试集的数据数量。

假设我们要训练参数，用来最小化损失函数，还是使用**平方误差函数**加上**L2 正则化**：

$$
J(\vec{w},b)=\frac{1}{2m_{train}}\sum_{i=1}^{m_{train}}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m_{train}}\sum_{j=1}^{n}w_{j}^{2}
$$

我们把训练集的 $J_{train}$ 算出来，再将成本函数最小化时的 $\vec{w},b$ 应用到测试集中，算出测试集的 $J_{test}$。

$$
J_{train}(\vec{w},b)=\frac{1}{2m_{train}}\sum_{i=1}^{m_{train}}(f_{\vec{w},b}(\vec{x}^{(i)}_{train})-y^{(i)}_{train})^2
$$

$$
J_{test}(\vec{w},b)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(f_{\vec{w},b}(\vec{x}^{(i)}_{test})-y^{(i)}_{test})^2
$$

如果 $J_{test}$ 的结果很高的话，说明这个模型不好。就比如房价预测时的过拟合状态，如下图所示。这就是平方误差成本的回归。

![7.3 坏模型](/machinelearning/three/07-03.png =360x)

### 1.2 分类模型的评测

应用到分类问题的话，我们拿手写数字识别（0 和 1）的案例来举例，使用逻辑回归的代价函数：

$$
J(\vec{w},b)=-\frac{1}{m}\sum_{i=1}^{m}\left[y^{(i)}\log{(f_{\vec{w},b}(\vec{x}^{(i)}))}+(1-y^{(i)})\log{(1-f_{\vec{w},b}(\vec{x}^{(i)}))}\right] + \frac{\lambda}{2m}\sum_{j=1}^{n}w_{j}^{2}
$$

$$
J_{train}(\vec{w},b)=-\frac{1}{m_{train}}\sum_{i=1}^{m_{train}}\left[y_{train}^{(i)}\log{(f_{\vec{w},b}(\vec{x}_{train}^{(i)}))}+(1-y_{train}^{(i)})\log{(1-f_{\vec{w},b}(\vec{x}_{train}^{(i)}))}\right]
$$

$$
J_{test}(\vec{w},b)=-\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}\left[y_{test}^{(i)}\log{(f_{\vec{w},b}(\vec{x}_{test}^{(i)}))}+(1-y_{test}^{(i)})\log{(1-f_{\vec{w},b}(\vec{x}_{test}^{(i)}))}\right]
$$

在分类问题中，相比于去比较成本函数的损失值，有另一种更简便的方法。最后的结果 y 是非 0 即 1 的，我们可以直接比较测试集算出来的预测 y 不等于真实的 y 的数量。如果数量过多，也是不好的模型。

### 1.3 交叉验证集（Cross Validation）

我们在上一小节的房价预测模型中知道了，训练集的损失函数 $J_{train}$ ，并不能确定它可以在测试集 $J_{test}$ 中进行泛化，或者说泛化到什么程度。那么当遇到像房价预测这样的线性回归模型的时候，我们应该怎么去选择模型呢？

![7.4 多个模型寻找最优 w,b](/machinelearning/three/07-04.png =560x)

假设我们去把 一阶多项式、二阶多项式、三阶多项式…… 罗列出来，分别进行成本函数的计算，然后将对应的 w，b 记录下来，带入到 $J_{test}$ 当中进行验证。

我们可以观测这些 $J_{test}$ ，看看哪一组 $w^{i},b^{i}$ 可以让我们的 $J_{test}$ 最小化。但是这个过程有个缺陷在于这有可能是迁移到 $J_{test}$ 时，对泛化误差的过度乐观估计。

==我们可以将数据集分成三个部分，60%为训练集，20%为交叉验证集，20%为测试集。== 有的地方交叉验证集（cross-validation set）也被称为验证集（validation）、开发集（development/dev set）。

![7.5 损失函数](/machinelearning/three/07-05.png =560x)

有了交叉验证集之后，我们将 $J_{train}$ 训练出来的 w 和 b 带入 $J_{cv}$ 中进行验证，而对于泛化的测试，我们通过 $J_{test}$ 来完成。这样一来，如果 $J_{cv}$ 给出的结果是 $w^{5},b^{5}$ 最好，并且 $J_{test}$ 给出的结果也同样是 $w^{5},b^{5}$ 最好，那么很大程度上可以表明它的迁移（泛化）效果是好的。反之泛化效果就不好。

这种方式可以用在很多模型中，包括神经网络模型，比如我们的手写数字识别。因为在训练的过程中，测试集的数据是不参与任何决策的，我们用交叉验证集代替了最初测试集做的工作，所以在最终进行泛化测试的时候，它是公平的，或者说是不过分乐观的估计。

## 2 通过偏差与方法进行诊断

开发机器学习系统的经典操作就是：你有了一个想法并训练了模型。但是发现它最后并没有像你希望的那样工作。（程序员的无情吐槽）

话是这么说，总是要解决问题的。查看机器学习算法的偏差和方差可以为你下一步的改进，提供很好的指导性建议。

我们在逻辑回归的章节中，介绍正则化减少过拟合的问题时，有过这样一张图，提到了“高偏差/欠拟合”、“高方差/过拟合”：

![7.6 高偏差与高方差](/machinelearning/three/07-06.png =560x)

现在回过头来看：

高偏差/欠拟合：训练集的损失函数就很大，并且验证集的损失函数还是很大。
刚刚好/可泛化：训练集的损失函数较小，并且验证集的损失函数也相对较小。
高方差/过拟合：训练集的损失函数很小，但是验证集的损失函数很大。

或者我们换一种方式来看：

![7.7 偏差方差与函数阶数](/machinelearning/three/07-07.png =560x)

从一阶多项式到高阶多项式的演变中，$J_{train}$ 是在不断变小的，而 $J_{cv}$ 是一个开口向上的二次函数，从大变小再变大，这就是为什么 d=2 时刚刚好，而不是 d=1 或者 d=4 。

还有一种“高偏差和高方差”同时存在的情况：训练集的偏差很高，同时验证集的方差也很高。这种情况一般是你有一个非常复杂的模型过度拟合，导致一部分过度拟合，一部分欠拟合，造成的结果。

### 2.1 lambda 对方差和偏差的影响

Lambda 这个值是正则化参数，它是权衡“保持参数 w 较小”与“较好的拟合训练数据”的变量。假设我们对一个四阶多项式进行正则化：

![7.8 Lambda对方差和偏差的影响](/machinelearning/three/07-08.png =560x)

如果 Lambda 非常大，那么算法就会积极的保证 w 足够小，所以最终得到的 w 的值会非常接近于 0，此时函数 f(x)的值约等于 b。相反，如果 Lambda 非常小（=0），相当于没有正则化，这时候图中这个四阶函数就会过拟合。

### 2.2 判断算法是否具有高偏差/高方差

我们在一些应用的实践中，可能会遇到这样的问题，比如**语音识别**，我们可能得到的 $J_{train}=10%$ ，也就是说，会有 10%的概率出错，但它一定就是高方差吗？

不一定，我们还要看其他很多因素，比如因为信号不好，导致声音被电流覆盖；比如因为在闹市中，自己的声音被周围人声音干扰…… 所以我们需要建立一个基准性能水平，也就是原始数据水平能够达到什么样子。

![7.9 基准性能水平](/machinelearning/three/07-09.png =560x)

因此，偏差需要观测：训练集的误差和基准性能水平是否相近。方差需要观测：验证集的误差和训练集的误差是否相近。

### 2.3 学习曲线

学习曲线是一种帮助你理解你的学习算法所拥有的经验量的函数，这里的经验是指，比如它所拥有的训练示例的数量。

![7.10 学习曲线](/machinelearning/three/07-10.png =560x)

我们可以看到，随着训练集数据量的增加，训练集的误差是会逐渐增大并趋于平稳的。增大是因为我们虽然可以根据数据的分布，去找到一个相对来说拟合的曲线，但是真实数据不可能完全符合数学模型中的函数，不可能一个 x 只对应一个 y，数据量大了，积少成多，它的偏差就会变大，但同样，它也会趋于稳定，并且验证集的误差也会降低并且趋于稳定。

但是需要注意，如果数据量的增大，并没有让你的训练集误差降低至基准性能水平附近，那很有可能是你的函数阶数不太对，就比如房价预测，我们一直使用直线（y=kx+b），它确实拟合不了数据。

<div class="layout">

![7.11 High bias](/machinelearning/three/07-11.png =360x)

![7.12 High variance](/machinelearning/three/07-12.png =360x)

</div>

而一个模型如果具有高方差（函数阶数太高，过拟合了），我们是可以通过增加数据量来解决这个问题，因为配合上一个较小的 Lambda，让函数正则化，可以让四阶函数，走势上趋近于二阶函数。

当然了，使用训练集的不同大小的子集来训练这么多不同的模型，在计算上是非常昂贵的，在实践中一般不会这样去做。之所以形成训练集的视觉图像，在一定程度上有助于我们思考，模型中的算法在做什么，是否具有高偏差或高方差。

## 3 小结回顾

### 3.1 下一步该怎么做？

还记得我们提出的问题吗，如果你的模型出现了大的误差，你该怎么做？我们总结一下提出的几个方法与对应的问题：

|     解决方式     |     对应问题     |
| :--------------: | :--------------: |
| 增加训练集数据量 | 高方差（过拟合） |
| 尝试减少特征数量 | 高方差（过拟合） |
|   创建新的特征   | 高偏差（欠拟合） |
| 改变多项式的阶数 | 高偏差（欠拟合） |
|   减小 Lambda    | 高偏差（欠拟合） |
|   增大 Lambda    | 高方差（过拟合） |

### 3.2 在神经网络中的应用

在神经网络出现之前，机器学习工程师对于偏差方差之间的权衡进行了很多的讨论，在这种权衡中，必须平衡多项式次数的复杂性。

![7.13 偏差与方差之间的权衡](/machinelearning/three/07-13.png =560x)

这个房价预测的案例还是简单的，因为只是一元函数，如果是多元 x，y，z……，对吧，越大的模型是越复杂的。

神经网络的出现，让我们摆脱了必须权衡偏差和方差与一些警告的困境。如果让你的神经网络足够大，这个模型几乎总能很好的适应你的训练集，只要训练集的数据量不超过硬件运算的限制。

这意味着它为我们提供了一种新方法，可以根据需要去尝试减少偏差或减少方差，而无需在两者之间进行权衡。

![7.14 偏差方差与神经网络](/machinelearning/three/07-14.png =560x)

减少偏差的一种办法：使用更大的神经网络（更多的隐藏层，或者每层更多的神经元）

减少方差的一种办法：获取更多的数据，返回并重新训练模型，再进行验证。

有人也许会问：如果神经网络过大，不会出现高方差的情况吗？事实证明，具有精心选择的正则化的大型神经网络，通常与较小的神经网络一样好用，甚至更好。

就比如手写数字识别，原先是 【25，15，1】这样的结构，换成【30，30，30，1】这样更大的结构，只要正则化 Lambda 调的好，同样也能够识别的很好。

唯一的缺点就是，大的模型会比较费时间、费硬件（显卡、GPU）。所以深度学习的兴起确实改变了机器学习从业者对于偏差和方差的思考方式，但知道这些同样对构建模型有帮助。

正则化在神经网络的代码中是 kernel_regularizer=? 这样表示的：

```py
# Unregularized MNIST model
layer_1 = Dense(units=25, activation="relu")

# Regularizer MNIST model
layer_1 = Dense(units=30, activation="relu", kernel_regularizer=L2(0.01))
```

## 4 机器学习的迭代开发

**1. 确定系统的总体架构**（选择你的机器学习模型、决定使用什么数据……）
**2. 训练模型**（几乎绝大多数的第一次训练都不是令人满意的）
**3. 实施、查看一些诊断**（例如偏差/方差，或者之后会提的错误分析）
**4. 根据诊断的结果，做出下一步决定**

吴恩达教授讲述了一个，他多年前参与过的一次反垃圾邮件会议的例子。垃圾邮件发送者有时会故意拼错诸如 watches、medicine、mortgages 之类的单词，以尝试让垃圾邮件识别器出错。

![7.15 垃圾邮件与正常邮件](/machinelearning/three/07-15.png =560x)

所以应该如何去构建分类器，来识别垃圾邮件和正常邮件呢？

一种方法是训练一个监督学习算法，其中输入的特征 x 是电子邮件的特征，输出的标签 y 将是 1 或 0，也就是垃圾邮件或正常邮件。

构建电子邮件特征的一种方法是，取英语或其他词典中排名前 10000 的单词，并使用它们来定义特征 x~1~、x~2~ 到 x~10000~。就如比，假设特征单词中存在这些蓝色单词，通过它出现的次数来构建向量，或者直接用出现的频率做统计。这样通过我们统计到的特征，再加上分类算法，就可以得出标签 y。

![7.16 词频统计](/machinelearning/three/07-16.png =560x)

训练完成之后，你发现过滤垃圾邮件的效果并不是那么好，那么你可能：

==1. 收集更多的垃圾邮件数据。== “Honeypot”project 蜜罐项目，创建大量的虚假电子邮件地址，并故意将这些虚假的电子邮件地址，交给垃圾邮件发送者手中，这样收到邮件时，我们就知道这是垃圾邮件。是一种获取大量垃圾邮件数据的手段。

==2. 开发基于电子邮件路由的更复杂的功能。== 路由相当于网络中的路径，邮件在发到收件人手中时，可能经过了很多个服务器中转站，通过 Email Header，所谓的电子邮件标头信息，可以跟踪电子邮件通过的不同服务器，不同网络的信息。有时这也可以判断出垃圾邮件。

==3. 对邮件内容做出更复杂的判断。== 比如 discounting 和 discount 可能是不同的意思，也可能是相同的意思；或者提出一种算法来检测（故意的）拼写错误。

下一步该做什么，选对了会事半功倍。就比如，如果算法具有高方差，收集更多数据可能会有大的帮助。但如果是高偏差，那么即使你不停的收集数据，也不能起到很大的帮助。所以，**遇事不决，请看模型评估小结。**

### 4.1 错误分析（Error Analysis）

错误/误差分析具体的来说就是从验证集中找到一组算法错误分类的示例，并寻找它们的共同点归类。假设你的验证集里有 500 个数据，结果算法出错了 100 个，误差分析就是手动的去查看这 100 个示例，并深入了解算法出错的地方。

举个例子：

1. 比如你注意到有很多的药品销售的邮件被错误的分类成垃圾邮件，手动的查看这个类别中有多少邮件是药品垃圾邮件，并写下来 21 件。

2. 比如你查看电子邮件的路由信息，发现有 7 封邮件路由异常，18 封电子邮件试图窃取密码，或者是网络钓鱼的电子邮件。

3. 垃圾邮件有时也是以图像的方式出现，它们并不在电子邮件正文中写垃圾信息，而是在图像中写入垃圾信息，假设有 5 件。还有 3 封故意拼写错误的邮件。

当你最终得到这些统计数据，你会发现：药品、试图窃取密码、网络钓鱼这些邮件似乎是个大问题，而故意拼写错误，只是一个较小的问题。

特别是，这个分析告诉你，即使你要构建非常复杂的算法来查找故意拼写错误，它也只能解决 100 个错误分类示例中的 3 个。所以即使要解决误差，故意拼写错误的优先级也会很低。

你或许需要获取更多的药品垃圾邮件的数据、或者想出一些与药物的特定名称组合新特征，来帮助算法更好的识别此类出现的垃圾邮件。

或许你会查看电子邮件中的 URL 并编写额外功能的特殊代码，以查看它是否链接到可疑的 URL。或者专门收集更多的网络钓鱼电子邮件数据。

### 4.2 添加更多数据

在训练机器学习算法时，感觉我们总是希望拥有更多的数据。有时我们会很想获取所有内容的更多数据，但是，尝试获取所有类型的更多数据，可能既缓慢又昂贵。

因此，专注于**添加经过分析表明的，可能有帮助的类型的数据**，是添加数据的一种方式。就比如上一小节邮件过滤中，专注于药品的垃圾邮件数据。

当然了，如果你能把所有类型的数据，都能搞到更多，那也是好的，没有问题。但如果只是子集，一部分的数据有问题，你想要提高性能，就要获取这些子集的类型的数据。

还有一种方法叫做 **数据增强**。就比如，一个字母 A，把它旋转、扩大、缩小、增加障碍物或者说对比度，再者镜像、扭曲…… 经过这样的操作之后，让机器还能识别出它是 A，这就起到了数据增强的作用。

<div class="layout">

![7.17 图像数据处理1](/machinelearning/three/07-17.png =360x)

![7.18 图像数据处理2](/machinelearning/three/07-18.png =360x)

</div>

这种思想同样也能用于语音识别中，比如我们随便说一句“今天的天气很好”，我们再建立几个数据集，比如：人潮拥挤的人群中，很多人说话的背景音；赛车场上很多引擎运行的轰鸣声；信号不太好的广播或者通话，造成的卡顿或者模糊。

一般来说，更推荐 **数据合成**，创建新的数据，而不是在原有的数据上进行修改。拿这个 OCR 图片文字识别来举例，我们要对多个区域内的文字进行提取。你会发现，这些海报中，字体千奇百怪，还有很多艺术字体，看起来好像很困难。

<div class="layout">

![7.19 OCR](/machinelearning/three/07-19.png =360x)

![7.20 真实数据与合成数据](/machinelearning/three/07-20.png =360x)

</div>

7.20 左边的图片是采集到的真实的数据，就是对图像进行切片分块，提取出最小的一个字母大小的方块。

而 7.20 右图，看上去十分的逼真的合成数据，它是怎么来的呢。我们电脑中也有很多字体类型，我们可以在文本编辑器中把它们进行扭曲、变换、缩放等，这样就得到了一个数据合成的模拟的数据集。

数据合成一般用于计算机视觉多一点，其他的领域相对较少。

在传统的机器学习中，我们都是以算法和模型为中心去做研究，感谢机器学习研究的范式，让线性回归、神经网络、决策树这些东西出现，它们有些内容在算法以及实际运用上面已经做的很好了，这让我们的工作重心，从算法和模型，转移到了数据收集上面。

希望数据增强、数据合成、错误分析得到的专有化类型数据，这些方式能让你的机器学习系统的性能得到进一步提升。

### 4.3 迁移学习（Transfer learning）

迁移学习的关键思想是从完全不同的/几乎不相关的任务中获取数据，使用神经网络让这些不同任务的数据，提高你的应用中的算法。

就比如我们现在有一个关于，猫、狗、车、人……的图片数据集一百万张，去做一个监督学习的分类预测，一共 5 个神经网络层，最后一层可能有 1000 种输出。

我们可以把它迁移到手写数字识别当中，输出层当然需要调整为 10 个输出，并且这一层的参数 w，b 是必须要重新训练的。

![7.21 迁移学习](/machinelearning/three/07-21.png =560x)

有两种方式：（1）只重新训练输出层的参数。（2）所有层的参数都重新训练。

迁移学习的好处就是：（1）除了输出层外，前面的神经网络层，都是以迁移前的参数作为基准，这样子在训练的过程中，只需要微调，会节省大量时间。（2）并且所需要的数据集数量会大幅度减少，比如人和物的检测识别用了一百万张图片，可能手写数字识别仅仅需要一万张，甚至一千张。

在《神经网络初探》中的“4.图像感知案例中”，提到了识别人脸、识别汽车这两个案例，观察一下你会发现，图像识别的底层逻辑几乎是通用的：

1. 首先是检测边缘线条
2. 然后检测角落，不同的线条组合起来的稍大一点的结构
3. 最后检测曲线，或者说基本形状

所以在不同类型的图像识别中，预训练的结果是可以做到迁移效果比较好的。==划重点：在相同类型的学习任务中，预训练的迁移效果是比较好的。== 如果你是要构建一个语音识别的任务，那你预训练去做图像的识别，效果就会很差。

## 5 机器学习项目的完整周期

**1. 定义项目。** 换句话说，决定项目是什么、你想做什么。比如我想做语音搜索中的语音识别，那就是对着手机说话，而不是在手机上打字。

**2. 收集数据。** 确定机器学习系统所需要的数据，比如着手获取音频，并获取数据集带有标签的转录本。

**3. 训练模型。** 按照例子来说，我将训练语音识别系统，实施 caravel 错误分析，并迭代改进我的模型。这里是一个循环的过程。

**4. 部署系统。** 在部署一个系统的时候，还必须确保能继续监控系统的性能，并维护系统以防止性能变差，而不是简单的在服务器上进行托管。

部署系统的一般流程是，我们将一个训练好的机器学习模型放入服务器（常被叫做推理服务器）中，开放它的接口，在移动应用程序里，调用这个接口。

推理服务器根据移动应用程序输入的 X 反复进行预测，然后返回预测结果 Y 到移动应用程序里。

![7.22 部署机器学习系统](/machinelearning/three/07-22.png =560x)

除了基本流程以外，你还可能需要软件工程来完成：

1. 根据应用的规模，确保你的推理服务器能够对不太高的计算成本做出可靠和有效的预测。（上百万人使用的应用，和几百人使用的应用，代价是不一样的）

2. 管理对于大量用户的扩展。

3. 你可能希望记录你获得输入 X 和预测 Y 的数据，假设用户允许你存储数据。

4. 如果你可以访问这些数据，它对于系统检测也十分有用。比如我建立了一个语音识别系统，但是政府的领导人换届了，系统中检测不到。因为可以访问数据，进行错误分析，所以可以很快的找到问题，重新训练模型，执行模型更新操作。

在机器学习中还有一个不断发展的领域叫做 MLOps 。它是指如何系统的构建、部署和维护机器学习系统的实践。这种实践，是为了确保你的机器学习模型可靠、可拓展、具有良好的规律、收到监控，然后你可以有机会根据需要对模型进行更新以保证其良好的运行。

### 5.1 公平偏差、伦理道德

在机器学习的历史上出现了很多系统，其中一些被广泛宣传，不幸的是，结果却表现出完全不可接受程度的偏差。

比如曾经的一个面部识别系统，相比于浅色皮肤的人，更频繁的将深色皮肤的人，与犯罪嫌疑人的面部照片相匹配。这显然是不合适的。

除了偏差和公平对待，还存在机器学习算法的不正当用例、负面用例：

![7.23 对奥巴马的造假视频](/machinelearning/three/07-23.png =560x)

比如曾经有一个公司，对美国前总统巴克拉奥巴马的视频，进行了深度造假的动态推送，现在应该还能找得到，虽然它是完全公开、完全透明的，但显然在未经同意的情况下使用这项技术生成虚假视频是不道德的。

有人的地方就有争斗，新闻舆论有造假的就有为真理抗争的，金融领域有诈骗的就有预防诈骗的……技术只是技术，我们没有办法要求别人，只希望看过这段文字的你们，不要用机器学习的技术去做让世界变差的事儿。

### 5.2 倾斜数据集的误差指标

如果你正在开发一个机器学习应用程序，其中正例和反例的比例非常倾斜，与 50-50 相去甚远，那么通常的错误指标（如准确度）会变得效果不佳。

假设你正在训练二元分类器，根据实验室测试结果或者患者的其他数据，检测患者是否患有罕见疾病。如果存在罕见疾病，则 Y=1，否则 Y=0。

你的算法模型的训练结果，错误率只有 1%，但是在你的数据集中，只有 0.5%的人患有罕见疾病。你会发现，不论你怎么预测，结果都会是 0，即不存在罕见疾病。

这么一说是不是突然感觉混乱了，原本是误差越小越好，现在拿了 0.5%、1%、1.5% 三个误差的模型出来，你反而不知道哪个更好，因为误差最小的预测可能不是特别有用的预测，就像罕见疾病的例子一样。

在处理倾斜数据集问题时，我们通常采用不同的误差度量，而不仅仅是分类误差来决定你算法的性能。一种常见的错误指标是：准确率和召回率。

![7.24 准确率和召回率](/machinelearning/three/07-24.png =560x)

首先我们要建立一个 2×2 的混淆矩阵，横向是真实分类，纵向是预测分类。预测正确的蓝色和绿色分别是真阳性和真阴性，预测错误的红色、黄色分别是假阳性、真阴性。

准确度是指“真阳性”除以“真阳性+假阳性”。
召回率是指“真阳性”除以“真阳性+假阴性”。

召回率可以很好的检测是否 Y 一直为 0 的情况，如果真阳性的数量为零，它永远也不会预测出阳性，所以召回率就等于 0 除以 实际阳性的数量，即等于 0。同理，这种情况下准确度也同样为 0。

### 5.3 准确度与召回率的权衡

高准确度意味着：如果患者被诊断出患有罕见疾病，那大概率是正确的诊断。

高召回率意味着：如果有患者患有这种罕见疾病，算法能正确的识别出他们确实有了这种疾病。

但是这两者能同时都高的机会很少，所以会出现很多权衡的时刻。拿诊断疾病举例：如果你使用逻辑回归进行预测，它会输出 0 到 1 之间的数字。一般来说，我们会把阈值放在 0.5 上，大于 0.5 预测为 1，小于 0.5 预测为 0。

![7.25 准确率和召回率的权衡](/machinelearning/three/07-25.png =560x)

假设我们的理念是，每当预测到患者患有罕见疾病，我们可能不得不将他们送去接受可能有创并且昂贵的治疗。如果这个病的后果不是那么糟糕（即使没有积极治疗），那么只有在我们非常有信心的情况下，我们才希望预测 Y=1。

在这种情况下，我们可以设置一个更高的阈值 0.7，只有 f(x) 大于等于 0.7 才预测为 1，小于 0.7 则预测为 0。提高阈值意味着准确度会提高，因为无论何时你去预测，你都更有可能是正确的，但是它也导致召回率较低。

假设我们想避免遗漏太多罕见病例，我们想要的是在有疑问的时候，预测 Y=1。也许治疗会痛苦或昂贵，但是不治疗会给患者带来更糟糕的后果，出于安全考虑，我们预测他们患有疾病，并考虑对他们治疗。

如果对于你的应用程序，这是更好的决策方式，你可以选择降低阈值，比如将其设置为 0.3。只要你认为疾病存在的可能性超过 30%，就预测 Y=1，只有当你非常确定该疾病不存在时，你才预测为 0。

对于许多应用程序，手动选择阈值以权衡准确度和召回率是你最终要做的。但也有一些自动权衡两者的指标，最常见的方法是计算 F1 分数（F1 score）。

![7.26 F1分数](/machinelearning/three/07-26.png =560x)

F1 分数是一种计算排序平均值的方法，它更强调的是两者之中较低的值。因为事实证明，如果算法的精度非常低，或者召回率非常低，那它就没什么用了。

F1 分数公式：

$$
F1\ score = \frac{1}{\frac{1}{2}(\frac{1}{P}+\frac{1}{R})} = \frac{2PR}{P+R}
$$

在数学中，这个方程也被称为 P 和 R 的调和平均数。
