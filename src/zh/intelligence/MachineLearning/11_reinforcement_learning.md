---
title: 3-3 强化学习
order: 11
author: AOSAI
date: 2024-08-28
category:
  - 机器学习
tag:
  - 强化学习
---

<style>
  @media (orientation:landscape){
    .layout{
      display:flex;
    }
  }
  @media (orientation:portrait){
    .layout{}
  }
</style>

强化学习（Reinforcement Learning）

## 1. 强化学习概述

在机器学习中，强化学习与其说是一类算法，不如说是一种思想，就和 **贪心、动态规划、分治、回溯** 这些经典的解决思路一样。虽然目前尚未在商业领域中得到广泛应用，但它也是机器学习的支柱之一。

我们还是从一个例子开始，图 11.1 是斯坦福大学自主研发的遥感直升机，重 32 磅。与其他遥感直升机一样，它配备了机载计算机、GPS、加速度计、陀螺仪和磁罗盘，因此它可以随时非常准确的知道自己的位置。

<div class="layout">

![11.1 强化学习-遥感直升机1](/machinelearning/five/11-01.png =360x)

![11.2 强化学习-遥感直升机2](/machinelearning/five/11-02.png =360x)

</div>

遥感直升机的正常操作，就像 90 年代用手柄打卡带游戏一样，通过操作两个摇杆，以及不同的功能按钮，保持直升机在空中的平衡和飞行。

图 11.2 是吴恩达教授驾驶摇杆直升机的图像，仔细观察会发现，这个直升机它在倒着飞，有点像空中杂技。没错，它就是用强化学习做到的。如果你有兴趣看更多的视频，可以 [点击此处](http://heli.stanford.edu)。

那么问题来了，如果给你一架摇杆直升机的密钥，让你来编写一个程序去自主驾驶它，你会怎么做？

### 1.1 什么是强化学习

假设我们给定一个任务：通过直升机的位置来决定如何移动驾驶杆。

我们将直升机的位置、方向、速度等称为状态 s。目标任务是找到一个函数，将直升机的状态映射到动作 a，即将两个操作杆推多远，以保证直升机在空中飞行时保持平衡不会坠毁。

这个任务也许能通过监督学习完成。比如我们有大量的状态观察结果，并且有一位专业的人类飞行员告诉我们应该采取的最佳行动是什么。然后你就可以使用监督学习训练神经网络，以直接学习 x（s）到标签 y（动作 a）的映射。

但事实证明，当直升机在空中移动时，“应该采取什么正确的行动”这个问题是很模糊的。比如向左倾斜时是一点还是很大？或者增加直升机压力是稍微还是很多？得到 x 和理想动作 y 的数据集是非常困难的。

这就是为什么对于很多控制机器人的任务，监督学习方法效果不佳，从而改为使用强化学习。**强化学习的一个关键输入，叫做奖励（函数），它会告诉算法什么时候做的好，什么时候做的不好。**

对于奖励函数，在吴恩达教授看来就像是训练小狗。如何让小狗表现良好呢？你不能向小狗展示太多东西，相反，你只是让它做自己的事情，如果做的好，就鼓励夸夸，如果做了坏事，就凶它骂它。然后希望它自己学习如何做更多好的事情，做更少坏的事情。

强化学习算法也是这样，做的好的时候夸你，做的不好的时候骂你。比如直升机飞的好的时候，奖励它多飞 1 秒（+1）；飞的不好的时候，给一个负奖励，少飞 1 秒（-1）；如果坠毁，给一个非常大的负奖励，比如（-1000）。

### 1.2 形式（火星探测器）

![11.3 火星探测器1](/machinelearning/five/11-03.png =560x)

在这个简化的例子中，探测器有 6 个可能会移动的位置（状态）。假设探测器一开始在状态 4，它可以去不同的地方用它的传感器（探头、雷达、光谱仪等等）来分析火星上不同地方的岩石，或者拍摄有趣的照片供科学家们观看。

状态 1 和状态 6 都有非常有趣的地质结构，科学家们希望探测器对其采样，但状态 1 的有趣程度为 100 分，状态 6 的有趣程度为 40 分。其余的状态奖励为 0。

在每一步的决策中，探测器都可以选择向左走或者向右走。我们可以模拟几种情况来做说明：

- 一直往左走，奖励为 [0, 0, 0, 100]
- 一直往右走，奖励为 [0, 0, 40]
- 先往右走一次，再往左走，奖励为 [0, 0, 0, 0, 0, 100]

第三种情况很明显不太好，但是它也有可能会发生。

总而言之，每一个阶段，探测机器人都会处于某种状态（称为 s），它可以选择一个动作（称为 a），并且它还有一些从状态中获得的奖励（称为 R(s)），以及因为动作而产生的新的状态（称为 s'）。

强化学习的核心要素就是这四件事：**状态、动作、奖励、下一个状态**，记录为：（s, a, R(s), s'）。比如探测器从状态 4 往左走一次：（4, left, 0, 3）。

对于这个应用程序，假设它进入状态 1 或 6 时，这一天就结束了。这种情况在强化学习中被称为**终端状态**，意味着一旦到达终端状态之一后，获得奖励后就结束后续的动作。

### 1.3 回报（火星探测器）

回报这个概念，是强化学习中如何实施动作，做决策的关键。

做个有意思的类比：假设你站在分叉路口，往左走 5 分钟可以捡到一张 5 美元的钞票，往右走 30 分钟可以捡到一张 10 美元的钞票，你会往哪里走？

虽然 10 美元看起来比 5 美元好多了，但是如果要你花 30 分钟去拿那张 10 美元，也许你会觉得没有 5 美元来的更方便。

所以，在这个例子中，回报的概念抓住了你“更快获得奖励”可能比“需要更长时间才能获得奖励”更有吸引力。

**回报被定义为这些奖励的总和，但其中需要一个叫做“折扣因子（Gamma）”的东西加权。** 还是火星探测器的图例：

![11.3 火星探测器1](/machinelearning/five/11-03.png =560x)

假设折扣因子为 0.9，我们从状态 4 一直向左移动到状态 1，它的回报是：

$$Return=0+(0.9)\cdot{0}+(0.9)^{2}\cdot{0}+(0.9)^{3}\cdot{100}=72.9$$

我们可以看到，折扣因子 Gamma 是指数型增长，越到后面越小。吴恩达教授的解释很有趣，他说：Gamma 的作用是让强化学习有点不耐烦。这样它就会往奖励越大的、越靠近的状态上靠。

用符号归纳一下回报的函数，它可以写为：

$$Return=R_{1}+\gamma{R_{2}}+\gamma^{2}{R_{3}}+\cdots$$

在许多强化学习算法中，折扣因子的选择是非常接近 1 的数字，比如 0.9，0.99，甚至 0.999。但是为了能从另一个角度，更好的理解回报的原理，这里 Gamma 选择了 0.5。

![11.4 火星探测器2](/machinelearning/five/11-04.png =560x)

图中的三行数据，每一行里红色的数字表示回报，黄色的箭头表示在该状态下往哪边走，黑色的数字代表奖励。三种不同的移动模式所带来的回报，差别是很明显的。

### 1.4 决策（火星探测器）

如同前几个小节所讲，强化学习中可以采取多种不同的方式，去决定下一个动作该做什么，比如在火星探测器中：

1. 总是追求更接近的奖励。靠近左边就往左边走，靠近右边就往右边走。
2. 总是追求更大的奖励。状态 1 奖励最大，所以总是往左走。
3. 总是追求更小的奖励。总是往右走，虽然看起来不是一个好主意，但也是一种选择。
4. 往更大的奖励走，除非距离较小的奖励仅一步之遥。状态 234 往左走，状态 5 往右走。

![11.5 火星探测器3](/machinelearning/five/11-05.png =560x)

在强化学习中，我们的目标是提出一个称为 策略 Pi 的函数，将任何状态 s 作为输入并将其映射到它希望我们采取的某个动作 a。比如策略 Pi 选择了第 4 种方式，那么探测机器人就会按照第 4 种方式去移动。

强化学习的目标是找到一个策略 Pi，告诉你在不同的状态下，采取什么行动可以获得最大化的回报。

### 1.5 回顾总结

我们用 6 种状态的火星探测器示例初步讲解了强化学习的形式，让我们快速回顾一下关键概念，并了解如何将这组概念应用于其他的程序。

![11.6 回顾小结](/machinelearning/five/11-06.png =560x)

火星探测器和遥感直升机已经在前面说过了，第三个是国际象棋。假设你想使用强化学习来学习下棋：

- **状态**：棋盘上所有棋子的位置（简化版）。
- **动作**：游戏中合法的移动。
- **奖励**：常见方式为，赢了奖励+1，输了负奖励-1，零奖励可能与游戏有关。
- **折扣因子**：国际象棋通常 Gamma 为接近 1 的数字，比如 0.99。
- **回报**：$R_{1}+\gamma{R_{2}}+\gamma^{2}{R_{3}} ...$
- **决策**：目标棋子被赋予了一个棋盘位置，使用策略 Pi 选择一个好的动作。

这种强化学习应用程序的形式实际上有一个名字，叫做：**马尔可夫决策过程（Markov Decision Process，MDP）**。它是指：未来仅取决于当前的状态，而不是取决于到达当前状态之前可能发生的任何事情。

## 2. 状态-动作价值函数

状态-动作价值函数（State-action value function）的目的是为了寻找当前状态下，回报最大的动作。也就是让回报最大化，它的形式可以写为循环：

Q(s, a) = Return if you

- start in state s.
- take action a (just once).
- then behave optimally after that.

![11.4 火星探测器2](/machinelearning/five/11-04.png =560x)

这是 1.3 小节中讲回报的图，第一行是火星探测车全向左走，第二行是全向右走。我们将这两行的回报结合起来，对比大小来看，第三行其实就是最大回报的动作决策。

另外，因为这个函数总是被写作 Q 函数，或者 Q\*函数（optimal Q function），所以如果你在很多文献里看到了它们，不要惊讶，它们就表示状态-动作价值函数。

### 2.1 代码示例

这是吴恩达教授简化的代码（[utils.py 文件可以从此处打开](/machinelearning/five/utils.py)），他希望我们通过自己修改一些参数，比如更改两边的奖励数值、更改折扣因子的大小等等，看看自动策略会如何根据这些不同的值而变化。

::: tabs

@tab 简化版代码

```py
import numpy as np
from utils import *

num_states = 6
num_actions = 2

terminal_left_reward = 100
terminal_right_reward = 40
each_step_reward = 0

# Discount factor
gamma = 0.5
#probability of going in the wrong direction
misstep_prob = 0

generate_visualization(terminal_left_reward, terminal_right_reward, each_step_reward, gamma, misstep_prob)
```

@tab 运行结果

![11.7 状态-动作价值函数](/machinelearning/five/11-07.png =560x)

:::

有的同学可能看了运行结果会问，为什么状态 2 和状态 3 向右走的回报，不是 2.5 和 5，反而是 12.5 和 6.25 呢。因为这两个状态向右走的策略是：先往右走一次，然后一直往左走。

Q 函数的目的是找寻回报最大化的动作，在状态 2 中，12.5 明显比 2.5 大，所以迂回的走法比一直向右走看起来更好，状态 3 同理。

此外，misstep_prob 这个参数详情，请看 2.3 小节，随机环境。

### 2.2 贝尔曼方程（Bellman Equation）

$$Q(s, a)=R(s)+\gamma\cdot{\max{Q(s', a')}}$$

贝尔曼方程简单的来说，就是告诉我们，当前状态下该动作的回报，是由（1）当前状态的奖励，也称作即时奖励；（2）折扣因子 × 下一个状态的回报最大的动作；两个部分组成。它其实可以看作是对回报的拆分：

$$
\begin{align}
Return &= R_{1}+\gamma{R_{2}}+\gamma^{2}{R_{3}}+\cdots   \\
       &= R_{1}+\gamma{[R_{2}+\gamma{R_{3}}+\cdots]} \\
       &= R(s)+\gamma\cdot{\max{Q(s', a')}}
\end{align}
$$

![11.8 贝尔曼方程](/machinelearning/five/11-08.png =560x)

图 11.8 中举了两个例子，从状态 2 往右走，计算出来 12.5 就是它的最大回报；从状态 4 往左走，12.5 是它的最大回报。

### 2.3 随机环境

在实践中，由于刮风、偏离航线、车轮打滑等等原因，许多机器人没有办法完全按照你的要求去做，所以结果并不会总是可靠。

比如，在火星探测器向左行驶的途中，可能会遇见岩石滑坡，或者地面真的很滑，导致它滑向了错误的方向。

我们可以模拟它出现错误的概率，比如 90%的概率会正常运行，10%的概率会遭遇意外情况。比如：

- 从状态 3 向左走，没有出错：[0, 0, 100]
- 从状态 3 向左走，在状态 2 时打滑了一次，回到了状态 3：[0, 0, 0, 0, 100]

我们会发现，加入概率之后啊，回报就不是一个准确的公式，而是无数个公式的集合，我们需要取它的平均值，或者说是期望，来重新测定每个状态，往左或往右走的回报。

$$
\begin{align}
Expected\_Return &= Average(R_{1}+\gamma{R_{2}}+\gamma^{2}{R_{3}}+\cdots) \\
                 &= E[R_{1}+\gamma{R_{2}}+\gamma^{2}{R_{3}}+\cdots]
\end{align}
$$

可以修改 2.1 小节中代码参数 misstep_prob，做对比观察。同理，对于贝尔曼方程，我们也需要改写为期望的形式。

$$Q(s, a) = R(s)+\gamma\cdot{E[\max{Q(s', a')}]}$$

## 3. 连续的状态空间

我们使用的简化版火星探测器，是一组离散的状态，它意味着探测器只可能处于 6 个位置中的一个。

但事实上，它可以处于大量连续位置中的任何一个。比如，一条横向的 6 公里长的直线，我们以米作为单位，向左或向右走时，[0, 6000]m 以内的任何数字都是有效的。

![11.9 连续的状态空间](/machinelearning/five/11-09.png =560x)

我们再举一个例子，比如正在行驶的卡车（玩具车），它需要考虑的就不只是一个状态了，比如，前后方向的位置 x，左右方向的位置 y，卡车行驶的方向 θ，以及前后方向的速度 $\dot{x}$，左右方向的速度 $\dot{y}$，行驶方向/角度变化的速度 $dot{\theta}$。

所以卡车的状态将包含由这 6 个符号组成的向量：$s=[x, y, \theta, \dot{x}, \dot{y}, \dot{\theta}]^{T}$，并且这些符号中的任何一个，都可以采用有效范围内的任何值。比如方向/角度的范围为 0 到 360°。

### 3.1 登月器

![11.10 登月器](/machinelearning/five/11-10.png =560x)

这是一个很有意思的模拟月球着陆的程序，每个时间点你可以有四种操作：

1. 什么都不做，让惯性和重力将着陆器拉向月球表面。
2. 启动左侧推进器，将着陆器推向右边移动。
3. 启动右侧推进器，将着陆器推向左边移动。
4. 启动底部的主推进器，减缓下降的速度。

你的任务就是随着时间的推移，不断地选择行动，让着陆器安全的降落在两个黄旗中间的位置。那么它的状态向量都包含些什么呢？

- x 和 y 表示在水平方向和垂直方向的位置
- $\dot{x}和\dot{y}$ 表示在横轴和纵轴上的移动速度
- $\theta$ 表示着陆器的角度，也就是机身的倾斜程度
- $\dot{\theta}$ 表示倾斜的速度，或者说角速度
- l 和 r 是两个布尔类型的变量，对应：左腿是否着地、右腿是否着地

它的奖励函数也很有趣，之前的火星探测器，只有两个状态存在奖励，而这次的着陆器有 7 种奖励，并且包括负奖励：

![11.11 登月器的奖励函数](/machinelearning/five/11-11.png =560x)

1. 假设两根黄旗中间有一个垫子，着陆器正在设法降落在上面，我们根据降落途中的飞行情况，给 100~140 的奖励；
2. 并且有一个额外的奖励，离垫子中心越近，奖励越高，离垫子中心越远，奖励越低；
3. 如果坠毁，奖励-100；
4. 软着陆成功，奖励+100；
5. 每条腿落地，都会奖励+10；
6. 假设我们鼓励它节省燃料，所以每次启动主引擎（主推进器）时，奖励-0.3；
7. 每次触发左侧或右侧推进器时，奖励-0.03。

这是一个中等复杂程度的奖励函数，设计者对真正你想要的行为进行了一些思考，并将其编入奖励函数中，激励更多你想要的行为。

当你自己构建一个强化学习应用程序时，通常你会花些心思来准确的指定你想要什么，你不想要什么，以及将其编入奖励函数。

总结一下，着陆器程序的任务是：在给定状态向量 $s=[x, y, \dot{x}, \dot{y}, \theta, \dot{\theta}, l, r]^{T}$ 的情况下，让决策 $\pi$ 选择一个最佳的动作 $a=\pi(s)$，使得回报最大化，折扣因子选定为 Gamma=0.985。

### 3.2 强化学习中的神经网络

用强化学习来解决登月器或者其它问题的一个关键思想是，我们要训练一个神经网络来计算或近似 state，action 的状态动作价值函数 Q。这反过来又会让我们选择一个好的行动。

![11.12 强化学习中的神经网络](/machinelearning/five/11-12.png =560x)

神经网络的输入包含了所有的状态和动作，状态向量已经写过了：$s=[x, y, \dot{x}, \dot{y}, \theta, \dot{\theta}, l, r]^{T}$；4 个动作我们可以通过 one-hot 进行编码：

- nothing，什么都不做，[1, 0, 0, 0]
- left，启动左侧推进器，[0, 1, 0, 0]
- main，启动主推进器，[0, 0, 1, 0]
- right，启动右侧推进器，[0, 0, 0, 1]

所以这个由 12 个数字组成的列表或者说向量（8 个状态，4 个动作），我们将其写作神经网络的输入 $\vec{x}$。

需要注意的是，我们并不是输入状态就直接让它输出动作（纯粹的监督学习），而是让它输出 Q 函数。神经网络在这里只是强化学习的一个部分。

对于 Q 函数而言，这种方式的效果很好。那么到这里，问题就变成了：如何训练一个神经网络来输出 Q(s, a) 函数？

### 3.3 构建训练集

首先，该方法将使用贝尔曼方程，来创建包含大量示例（x，y）的训练集，然后使用监督学习中的神经网络做模型训练，将 state-action（$\vec{x}$）映射到目标值 Q(s, a)（y）。

![11.13 通过贝尔曼方程构建训练集](/machinelearning/five/11-13.png =560x)

我们将贝尔曼方程切分为两个部分，等式左边就是输入向量 x，等式右边就是输出 y。

问题的关键是，我们并不知道什么才是最佳的 Q 函数，能够将回报最大化的下一个状态和动作是什么我们不清楚。

没关系，我们可以取随机值，来构建一个，比如包含 1 万个数据的训练集。

### 3.4 Deep Q-Network（DQN）

通过前面几个小节，我们可以概括出神经网络构建的全过程：

**1. 初始化神经网络，随机的猜测 Q(s, a)。**

这就有点像训练线性回归模型时，随机的初始化所有参数，然后使用梯度下降一步一步完善。所以重要的其实是，算法能不能慢慢的改进参数，以获得更好的估计。

**2. 反复执行以下操作：**

2.1 在着陆期间，执行任何的操作，无论是好的还是坏的，你会获得多个这样的元组 $(s, a, R(s), s')$

2.2 重放缓冲区（Replay Buffer）：储存最新的 1 万个元组数据。

2.3 训练神经网络：将这 1 万个元组数据，通过贝尔曼方程构建成数据集。$maxQ(s', a')$ 最开始是随机初始化的，没关系，通过训练慢慢就会变成 $Q_{new}(s, a)=R(s)+\gamma\cdot{\max{Q(s', a')}} \approx{y}$

2.4 让 $Q=Q_{new}$

我们在反复执行训练神经网路的步骤中，Q(s, a) 是会继承上一次的训练结果的，所以每一次迭代，它都会变成更好的估计。

所以理论上来讲，只要你迭代的次数够多，训练的时间足够长，这个模拟着陆程序中的 DQN 就会变得足够好。

## 4. 算法改进

### 4.1 神经网络架构改进

![11.12 强化学习中的神经网络](/machinelearning/five/11-12.png =560x)

这是原本的神经网络模型，我们需要在每个状态上，分别对四个动作进行推理。也就是一个状态就要运行四次神经网络，或者说每个动作都对应一个神经网络，这是非常低效的。

![11.14 神经网络架构改进](/machinelearning/five/11-14.png =560x)

事实证明，在一个神经网络上同时计算并输出四个动作所对应的 Q 函数，效果会比原本的更好。因为通过贝尔曼方程，对比四个动作的回报，一次就可以知道在该状态下，做什么动作是最好的。

### 4.2 ε-贪婪策略（Epsilon-greedy policy）

当你处于某些状态时，可能并不想完全随机的采取行动，因为那样通常可能会是一个糟糕的结果。

我们原本的选项是：（1）选择一个动作 a，尽可能的最大化 Q(s, a)。即使它可能不尽人意，但是算法会尽力的使用我们当前对 Q(s, a)的猜测，并最大化它的收益。

而现在的选项是：**（1）0.95 的概率，选择一个能最大化 Q(s, a)的动作 a。（2）0.05 的概率，随机选择一个动作 a。**

这么做的原因是，假设在随机初始化 Q(s ,a)的时候，出现了一些不好的情况，比如 Q(s, a)始终都很低，这可能导致该启动主推进器时，一直不启动的类似问题。

引入一个 0.05 的概率，神经网络可以学会克服自己的先入之见，也许自己之前坚持的才是错的呢？

这种随机选择动作的想法有时被称为探索步骤（Exploration Step）。还有一个很神奇的地方是，贪婪指的是 0.95 的概率，寻求 Q(s, a)最大化的部分，0.05 的随机探索才是不贪婪的。

一般来说，ε 的值是从大到小变化的。最初，你可能采取较多的随机行动，因为本身也不知道哪个好，哪个不好；

随着时间的推移，Q(s, a)迭代很多次后，已经有了很多好的推理，你可能会更倾向于用新的 Q(s, a)来估计和选择行动，所以随机行动就降低了。

### 4.3 超参数的敏感性

在监督学习中，超参数是不那么敏感的，比如，学习率选择的不好，可能也就是会让训练量或者说训练时间翻个两三倍。

但是强化学习中，超参数是很敏感的，假设你的 ε-贪婪策略中，epsilon 的值选的不够好，训练量可能会翻个 10 倍、100 倍。这非常恐怖。

吴恩达教授觉得，这可能是因为强化学习算法本身还不够完善导致的。目前，对于一个新项目而言，还需要我们自己试错。

### 4.4 小批量处理（mini-batch）

小批量处理可以加速强化学习和监督学习的训练速度，但是前提是你的训练集真的非常大，我们从监督学习的线性回归说起：

![11.15 小批量处理1](/machinelearning/five/11-15.png =560x)

这是最初的房价预测的线性回归模型，但是假设有 1 亿个训练样本，本来在梯度下降时，更新参数 w 和 b 就要循环迭代很多次，再乘一个 1 亿的样本基数，训练时间会大到难以想想。

==小批量处理要做的事情就是，将其划分为不同的集合，一个集合中 1000 个样本、5000 个样本……**然后在进行成本函数计算时，第一次用集合 1 的样本，第二次用集合 2 的样本，依次类推**。==

![11.16 小批量处理2](/machinelearning/five/11-16.png =560x)

一次性处理（Batch learning）的结果是，梯度会直接的向最小的地方前进，但是数据量太大了，计算时间会非常长。

小批量处理（Mini-Batch learning）的优势在于，计算成本要低得多，所以当数据集很大的时候，它是一个更快的算法。一般会和别的算法一起用，比如 adam。

缺点也很明显，可能会因为不同集合的样本走歪路，虽然它也会最终趋向于全局最优化，但是它不是很可靠，并且会有噪点。

![11.17 小批量处理3](/machinelearning/five/11-17.png =560x)

回到强化学习中来，我们可以在训练神经网络模型的时候，使用小批量处理，将 1 万个数据，切分成 1000 个数据的不同集合。循环使用不同的集合来训练参数。

### 4.5 软更新

软更新可以帮助你的强化学习算法更好的收敛到一个好的解决方案。

在登月器的最后一步 $Q=Q_{new}$ 中，有一个隐患，假如遇到了像小批量处理时，数据集不好的情况下，新的 Q 值可能会更糟糕。

![11.18 软更新](/machinelearning/five/11-18.png =560x)

我们可以给新的参数乘一个 0.01，旧的参数乘一个 0.99，这就是软更新，因为我们只会接受一点点的新值，所以出现错误的可能性大大降低，并且收敛性也会变得更佳。
