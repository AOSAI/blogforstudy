---
title: 无监督学习（Unsupervised Learning）
order: 9
author: AOSAI
date: 2024-08-16
category:
  - 机器学习
tag:
  - 无监督学习
  - 聚类算法
  - 异常检测算法
---

<style>
  @media (orientation:landscape){
    .layout{
      display:flex;
    }
  }
  @media (orientation:portrait){
    .layout{}
  }
</style>

在这一章中，将会提到**聚类算法**、**异常检测**两种无监督学习算法。这两个技术都是当今许多公司在重要商业领域中使用的技术。

## 1. 什么是聚类

**聚类算法：查看大量数据点，并自动找到彼此相关或相似的数据点**。具体什么意思呢？我们来对比一下聚类算法和分类算法：

<div class="layout">

![9.1 分类算法](/machinelearning/four/09-01.png =360x)

![9.2 聚类算法](/machinelearning/four/09-02.png =360x)

</div>

图 9.1 是一个分类算法。数据集中给定了两个特征 x1 和 x2，以及对应的目标标签 y，通过逻辑回归算法或者神经网络，划分出这样一个边界，通过这个边界，对新的数据进行预测。

聚类算法（clustering），数据集中只包含特征 x 向量，而没有目标标签 y，因此我们无法让该算法预测我们想要预测的“正确答案”。相反，我们要求算法找到有关数据的有趣内容。比如通过某些方法，寻找到关于特征的一些有趣结构，看看能不能给他们分组。

图 9.2 是一个聚类算法，很明显 10 个数据被分成了两块，聚集成了两个集群的数据。

### 1.1 聚类算法的一些应用场景

![9.3 聚类算法的应用](/machinelearning/four/09-03.png =560x)

1. 在新闻网站上，聚合一些相似的新闻。

2. 在代码网站上，比如牛客网、leetcode，有些人可能是想学习技能，有的人可能是为了找工作，有些人或许是想知道有什么 AI 模型可以对自己的工作产生帮助。我们可以通过聚类算法，将这些有相同目标的人划分到一起。

3. 在 DNA 分析中，也常使用聚类算法，来找出一些蛋白质结构相同的特点。

4. 在天文学领域中，寻找一些行星相关联的特性。

## 2. K-means 算法

K-means 算法是聚类问题中，一个常用的算法。下面会通过原理、步骤、成本函数，以及一些构建 K-means 的技巧，来介绍这个算法。

### 2.1 K-means 原理

我们还是从举例子开始，假设我们有 30 个黄色的未标记的数据，分布如下图所示，我们要求它尝试找到两个集群。

![9.4 K-means原理-1](/machinelearning/four/09-04.png =560x)

（1）图 9.4 中的红叉和蓝叉，代表随机生成的两个集群的中心位置，它们被称之为**簇质心**。

![9.5 K-means原理-2](/machinelearning/four/09-05.png =560x)

（2）随后我们分别计算，所有数据和两个簇质心之间的距离，然后进行比较，将靠近红叉的数据标记为红色，将靠近蓝叉的数据标记为蓝色。

![9.6 K-means原理-3](/machinelearning/four/09-06.png =560x)

（3）分别计算红色数据和蓝色数据的平均值，记为 k1 和 k2。将簇质心红叉和蓝叉分别移动到 k1 和 k2 的位置上。

做完了一遍这些工作后，我们发现，新的簇质心会导致一些数据与其的距离产生变化，所以我们重复（2）和（3）的步骤，继续划分集群和移动簇质心，如下图所示，直到簇质心的位置不再变化。

<div class="layout">

![9.7 K-means原理-4](/machinelearning/four/09-07.png =360x)

![9.8 K-means原理-5](/machinelearning/four/09-08.png =360x)

![9.9 K-means原理-6](/machinelearning/four/09-09.png =360x)

</div>

### 2.2 K-means 算法

总结一下 2.1 小节内容，K-means 算法可以分为 3 个步骤：

**1. 随机猜测你可能要求它找到的两个（或几个）聚类的中心位置。**

**2. 遍历每一个点（数据），并查看它是更接近于哪个中心位置（簇质心）。打上不同的记号（在这个例子中用颜色进行区分）。**

**3. 通过取“不同记号数据”的平均值，重新计算簇质心的落点，并将其移动过去。**

其中 2 和 3 会重复进行。循环到簇质心的位置不再变化，数学上称之为收敛，就结束循环。

![9.10 K-means算法伪代码](/machinelearning/four/09-10.png =560x)

需要注意的是，聚类算法中有一个极端的情况，比如图中随机生成的两个簇质心，蓝色的没有数据最靠近它，它会计算 0 个点位的平均值，很明显会出错。

- 解决办法 1：减少簇质点的数量（k=k-1）
- 解决办法 2：从随机分配的初始化步骤重新开始

除此之外，聚类算法对簇群区别不是特别明显的数据也适用：

![9.11 K-means算法适用性](/machinelearning/four/09-11.png =560x)

左边是区分很明显的数据，右边是连续的、区分不明显的数据。

### 2.3 成本函数

在之前的很多监督学习算法的学习中，使用了梯度下降或其他的一些算法来优化成本函数。

而在上一小节中，我们发现 **关键均值算法（K-means）** 的簇质心是不断移动的，本质上也是在优化特定的成本函数。在这里，成本函数还有另一个名字，叫做 **失真函数（distortion function）**。

我们先来看一下成本函数的方程式：

$$
J\left( c^{(1)},...,c^{(m)};\mu_{1},...,\mu_{m} \right) = \frac{1}{m}\sum_{i=1}^{m}\|x^{i}-\mu_{c^{(i)}}\|
$$

- $c^{i}$ 代表分配给训练示例 $x^{i}$ 的索引数字；
- $\mu_{k}$ 表示集群质心 k 的位置；
- $\mu_{c^{(i)}}$ 意味着该索引的数据已经分配到某个集群的集群质心，表示目前该数据所对应的集群质心。

K-means 算法试图所做的，首先是找到聚类质心点的分配，其次是找到最小化平方距离的聚类质心的位置。

![9.12 成本函数](/machinelearning/four/09-12.png =560x)

数据关于质心的分配，就是计算距离，看看该数据离哪一个质心更近。虽然这样做，保证了单独的每一个数据，离所属的质心最近，但是总体的方差却不是最小的。

移动簇质心的工作，其实就是将簇质心与其对应的数据之间的平方距离之和（总方差）最小化，由此可以得到这组数据的最中心的位置。

如下图所示，两个数据分别位于 1 和 11，而质心位于 2，此时的总方差为 41；1 和 11 的中心点位于 6，若将质心移动至此，总方差为 25，已经达到了最小。

![9.13 移动簇质心](/machinelearning/four/09-13.png =560x)

**判断损失函数是否收敛**的办法其实和梯度下降差不多：

1. 一段时间内，成本函数的大小已经不再变化了。
2. 成本函数的下降速度已经变得非常非常慢。

### 2.4 初始化 K-means

簇质心的数量一定是小于训练集数据大小 K 的，因为我们的目的是找到某些数据的相似之处，然后给这些数据划分成一个集群。如果集群的数量都大于数据数量了，就失去意义了。

在 2.2 小节中，我们是通过随机生成质心位置来完成初始化的。但其实，通过随机选取数据集中的数据，来完成初始化，更加的实用。

![9.14 初始化 K-means](/machinelearning/four/09-14.png =560x)

但这种方式，也会存在一些问题，比如可能会陷入局部最优解的境地。如上图所示，因为初始化质心的差异，J1 到 J3 都是局部最优，但是我们通过观察图可以知道，J1 才是最优的划分情况。

**我们可以通过多次随机初始化的方式，来对比不同的方式的总体方差。一般来说，初始化的次数是 50 到 1000 次，如果数据量不太大，最好在 50 到 100 次左右。**

J2 中蓝色的质心，它的方差会比红色和绿色大得多；而 J3 中红色质心的方差会比蓝色和绿色大得多；只有 J1 中的三个质心的方差相差不大。因此，通过对比，我们知道 J1 才是最优解。

### 2.5 选择聚类数量

上一小节，我们探讨了簇质心的初始化位置如何分配，那么一个具体的问题当中，需要多少个聚类数量（簇质心）呢？

![9.15 选择聚类数量](/machinelearning/four/09-15.png =560x)

这个问题的答案其实是模糊的，因为没有标准答案。就比如上图给出的这一组数据，有的人来看，会划分成两个聚类（蓝色圈）；有的人来看，则可以划分成四个聚类（红色圈）。这两个答案都是对的。

如果你看过很多关于聚类的文献，你可能会找到一种叫做 **肘部方法（Elbow method）** 的算法，它可以自动的寻找合适的聚类数量，如下图中左边的图例所示。

![9.16 肘部方法](/machinelearning/four/09-16.png =560x)

肘部方法认为，聚类的数量增加到某一个值的时候，会像弯曲的手肘一样，导致成本函数下降的速度变得缓慢，因此，位于肘部的这个值，就是最佳的聚类数量。

**但是**吴恩达教授说自己很少用这个方法，他认为成本函数和聚类数量的关系，大多数时候都是一条圆滑的曲线，没有肘部位置的存在。**并且**我们不能以聚类数量为标准，降低成本函数，通过右边图例的曲线，我们知道以聚类数量为标准，只会让聚类数量不断增加，最终等于训练示例数量。

因此，如何选择聚类的数量，要从具体的下游任务进行分析。比如：T 恤的大小如何进行划分。

![9.17 T恤示例的聚类数量划分](/machinelearning/four/09-17.png =560x)

我们可以分成三个聚类（S，M，L），也可以分成五个聚类（XS，S，M，L，XL）。我们衡量的标准可以是“哪种聚类数量的划分让衣服穿着更合身”，亦或许要考虑“制作更多尺码的 T 恤所需的额外成本”。

还有一个很有趣的视觉应用，是 K-means 算法在图像压缩中的应用。图像压缩中会有一个权衡点，即“压缩后的图像看起来有多好”。你可以通过对比，选择你觉得适合的聚类数量，来决定压缩后的图像大小有多大。

## 3. 异常检测算法

异常检测算法会查看未标记的正常事件数据集，从而学会检测异常事件，或发出危险信号。

举个例子，像飞机这种大型运输工具，如果出现发动机故障会产生非常负面的结果，所以有些科学家在研究使用“异常检测”来检测制造的飞机发动机，是否看起来有异常，或者它是否存在某种问题。

初步的想法是这样的，当飞机发动机停止运行后，它可能存在以下特征：

- x1=heat generated（发动机产生的热量）
- x2=vibration intensity（振动强度）
- and so on（等等一系列的特征）

为了降低解释原理的难度，我们只用两个特征。我们知道发动机的制造肯定是很精细的过程，所以我们默认制造出来的发动机，都是没有问题的。

因此，我们通过获取某个飞机公司的许可，收集 m 台飞机发动机的运行特征 x1 和 x2。异常检测算法的工作是：看到这 m 个例子之后，概括出飞机引擎通常产生多少热量，以及他们的振动强度是多少。

如果一个全新的飞机发动机要下线，并且有一个 xtest 的新特征向量，我们想知道这个发动机看起来是否跟之前 m 台发动机相似。

![9.18 异常检测-发动机1](/machinelearning/four/09-18.png =560x)

### 3.1 密度估计（density estimation）

执行异常检测的最常见的方法是一个被称为密度估计的技术。当它获得一个样本的训练集时，它做的第一件事就是为 x 的概率建立一个模型。

换句话说，学习算法将会通过特征值 x1 和 x2 来尝试找出，哪些值在数据集中出现的概率大，哪些值在数据集中出现的概率较小。

![9.19 异常检测-发动机2](/machinelearning/four/09-19.png =560x)

通过图中模拟的三层椭圆，我们可以知道从里向外，飞机引擎正常的可能性逐渐降低。假设我们给出一个阈值 ε，也就是途中最外层的蓝色椭圆。

如果测试的数据概率小于阈值 ε（落在了篮圈外面），算法会觉得这个引擎可能存在问题；如果测试数据的概率大于阈值 ε（落在篮圈内），算法会觉得这个引擎看起来好像是正常的。

密度估计的常见应用：

1. 检测新生产的产品是否存在缺陷
2. 检测金融网站是否存在欺诈问题
3. 检测计算机的元件功率是否正常

### 3.2 高斯分布（Gaussian distribution）

为了应用异常检测，我们需要使用高斯分布，也称为正态分布（The normal distribution）、或者钟形分布（The bell-shaped distribution）。

![9.20 高斯分布](/machinelearning/four/09-20.png =560x)

我们先看右边这个图，假设我们有 100 个数据（x，y），并且它们符合正态分布，用曲线将直方图的顶点连接起来，就像是一个钟形的曲线，如同左边的图一样。

正态分布中的 μ 代表均值，σ 代表标准差（σ 的平方才是方差）。正态分布中，对于某个点的概率预测公式如下：

$$
p(x)=\frac{1}{\sqrt{2\pi}}e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}}
$$

$$
\mu = \frac{1}{m}\sum_{i=1}^{m}x^{i}
$$

$$
\sigma^{2} = \frac{1}{m}\sum{\left(x^{(i)}-\mu\right)}
$$

**高斯分布的一个性质为：概率总和永远等于 1 。** 因此图像的面积是不会变的。μ 控制图像的中心点在 x 轴的某处，而 σ 控制图像的宽度：

- σ 越大，图像越宽，高度越低；
- σ 越小，图像越窄，高度越高。

![9.21 高斯分布的性质](/machinelearning/four/09-21.png =560x)

除了上述的性质之外，还有一个很重要的性质：某一个测试数据

- 落在 [ μ-σ，μ+σ ] 中的概率为 67%
- 落在 [ μ-2σ，μ+2σ ] 中的概率为 95%
- 落在 [ μ-3σ，μ+3σ ] 中的概率为 99.7%

通过这样一个性质，结合上一小节的内容，该怎么结合实际情况对阈值 ε 选择，我们似乎已经有了思路。

### 3.3 异常检测算法

假设我们训练集中有 m 个数据 $(x_{1},x_{2},...,x_{m})$ ，每个数据有 n 个特征。特征之间都是独立的，所以它的概率总和应该是所有特征的对应概率的乘积。（概率论与数理统计中的内容），用公式表示为：

$$
\begin{align}
p(x) &=p(x_{1};\mu_{1},\sigma_{1}^{2}) \times\cdots\times p(x_{n};\mu_{n},\sigma_{n}^{2})  \\
     &= \prod_{j=1}^{n}p(x_{j};\mu_{j},\sigma_{j}^{2})
\end{align}
$$

基于这个新公式，我们来总结一下异常检测算法的步骤：

**1. 首先选择 n 个你认为对异常检测有帮助的特征。**

**2. 用高斯分布拟合所有的参数 $\mu_{1},...,\mu_{n};\sigma_{1}^{2},...,\sigma_{n}^{2}$**

$$
\mu = \frac{1}{m}\sum_{i=1}^{m}x^{i} \qquad
\sigma^{2} = \frac{1}{m}\sum{\left(x^{(i)}-\mu\right)}
$$

**3. 用训练好的参数，计算一个测试集新数据的概率**

$$
p(x) = \prod_{j=1}^{n}p(x_{j};\mu_{j},\sigma_{j}^{2}) = \prod_{j=1}^{n}\frac{1}{\sqrt{2\pi}\sigma_{j}}e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}}
$$

**4. 用概率和阈值 ε 做比较，小于则有异常，大于则无异常**

### 3.4 实数评估（Real number evaluation）

当你在开发学习算法时，比如选择不同的特征，或尝试不同的参数值（例如阈值 ε），你可能会有“是否以某种方式更改特征”、“增加或减少 epsilon（ε）或者其它参数”这样的决策。

那么改变之后的结果好不好，怎么判断？如果你有一种评估学习算法的方法，会比你训练很多次，然后亲自去对比数据容易得多。

这种方法有时候被称为**实数评估**。它可以用某种方式快速改变算法（比如改变一个特征或参数），并且有一种计算数字的方法可以告诉你这种改变是好是坏。

在异常检测中实数评估是一个经常被使用的方法。假设你多来年收集了 1 万个良好或正常的发动机数据，并且也收集到了 20 个有缺陷或异常的发动机数据。

我们将其做如下的划分：

- 训练集：6000 个良好引擎
- 交叉验证集：2000 个良好引擎（y=0），10 个异常引擎（y=1）
- 测试集：2000 个良好引擎（y=0），10 个异常引擎（y=1）

然后在训练集上训练算法，用高斯分布拟合这 6000 个数据，再在交叉验证集上，观察它正确标记了多少异常引擎。

我们可以通过调整交叉验证集的参数 epsilon（ε），将其调高或者降低，观察什么时候能够可靠的检测到这 10 个异常引擎。

再通过测试集，对交叉验证集的参数进行评估，看看它发现了 10 个异常引擎中的多少个，以及有没有将良好的引擎错误的标记为异常引擎。

如果你的异常引擎数据很少，比如只有 2 个，那么完全可以不要测试集，只保留训练集和交叉验证集。尽管这样做，没有办法对参数进行验证，但是当数据集太小的时候，这可能是最好的选择。

### 3.5 对比监督学习

**监督学习算法的本质是，希望从大量的正确示例中，学习到相关特征，然后预测与其相似的数据。**

比如，垃圾邮件归类的问题，我们通过给大量的垃圾邮件进行标记（y=1），让算法对特定类型的垃圾邮件有一个认识，然后对新的邮件进行检测，看其是否和学习到的垃圾邮件相似。如果相似，就将其标记为垃圾邮件。

**异常检测算法的本质，是排除异常的数据，是检测新数据与正常数据不一样的程度。**

比如，生产的零件质量检测，我们通过对大量优质的零件进行学习，用高斯分布拟合这些数据。如果新的零件，不符合这些优质零件的特征，就发出异常警告。

通过对比，我们知道监督学习算法也是可以做缺陷检测的，但是只能对已知的，有足够数据集的，也就是常见的缺陷进行检测。

而异常检测算法可以对未知的，不确定的错误进行排查。如果你觉得可能还会有某些想象不到的错误会出现，那么用异常检测算法会很有效。

### 3.6 特征选择

监督学习算法中，即使你有一些无关紧要的特征，或者选择的特征不完全正确，结果通常也不会有什么问题。

因为它必须有监督信号，该信号足以告诉算法，哪些特征可以忽略，哪些特征需要缩放等等，可以充分的利用提供的特征。

但是异常检测是无监督学习，只能从未标记的数据中进行学习，它很难找出需要忽略的特征。所以==仔细选择特征对于异常检测算法来说很重要。==

**（1）让特征尽可能的拟合高斯分布。**

![9.22 特征变换拟合高斯分布](/machinelearning/four/09-22.png =560x)

我们可以通过线性变换，让某些不符合高斯分布的特征，变成符合的。

比如：log(x)，或者 log(x+C)，C 表示常数。
比如：开根号，平方根，立方根……

要注意的是，无论你对训练集使用了什么样的转换方式，对交叉验证集和测试集的数据，也应用同样的方式转换。

**（2）通过错误分析对某些特征进行调整。**

假设有某个数据，它的正常概率和异常概率都很大。通常我们会尝试查看该数据，并找到我认为的什么地方是异常的，即使这个数据的特征看起来和其它的并没有什么不同。

如果我能够识别出一些新特征，那将有助于这个数据与普通的数据区分开来。然后添加该特征，可以帮助提高算法的性能。
