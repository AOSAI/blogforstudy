---
title: 1-3 分类问题
order: 4
author: AOSAI
date: 2023-11-11
category:
  - 机器学习
tag:
  - 机器学习
---

分类问题（Classification）

## 1.分类问题之逻辑回归

### 1.1 动机（Motivation）

在最初讲机器学习概念的时候，分类问题吴教授还举过一个例子，我们的 e-mail 邮件系统。它会有一些分辨性的功能，比如：

1. 这封电子邮件是垃圾邮件吗？（Is this email spam?）
2. 这封电子邮件是否带星标？（Whether the email is starred?）

或者说一些其他行业的判断问题：

3. 是否涉及交易欺诈？（Is the transaction fraudulent?）
4. 肿瘤是否是恶性的？（Is the tumor malignant?）

这类问题我们的答案只有 yes or no 这两种选项，对应我们编程来说，就是布尔类型变量，True or False。在机器学习里，它叫做 “二元分类问题”（binary classification），我们也可以把 True 当作 “积极类别”（positive class），把 False 当作 “消极类别”（negative class）

在这一类问题中，我们是没有办法单纯的用线性函数去表示的，因为线性函数表示的是同类型的东西的某种趋势，而分类问题它注定是要割裂开的，有点像分段函数，但又不完全一样。所以要引入新知识了，逻辑回归。

### 1.2 逻辑回归（Logistic Regression）

逻辑回归假设数据服从伯努利分布，通过极大似然估计的方法，运用梯度下降来求解参数，达到讲数据二分类的目的。逻辑回归虽然是一个非线性模型，但是其背后是以线性回归为理论支撑的。

在编程语言中，True 可以用 1 表示，False 可以用 0 表示，试想一下有没有一种曲线可以表示 1 和 0 的，别说，还真有，数学知识以一种卑鄙的形式进入了脑子。

$$
g(z)=\frac{1}{1+e^{-z}}, \quad 0<g(z)<1 \\
\quad \\
f_{\vec{w},b}(\vec{x})=g(\vec{w}\cdot\vec{x}+b)=\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}
$$

这就是逻辑回归的公式，我们来看一下图解：

![4.1 逻辑回归](/machinelearning/one/04-1.png =560x)

举个例子，还是肿瘤问题，假设 x 是肿瘤大小，y=0 时表示非恶性，也就是良性，y=1 时表示恶性，当 $f_{\vec{w},b}(\vec{x})=0.7$ 时，表示有 70%的概率肿瘤是恶性的。

恶性肿瘤预测的输入可以写作：$f_{\vec{w},b}(\vec{x})=P(y=1|\vec{x};\vec{w},b)$，此时的输入 input 为 $\vec{x}$，参数 parameters 为 $\vec{w},b$，概率 Probability 为 y=1 表示预测肿瘤为恶性时的概率。

要注意的是，良性和恶性两者的概率和是等于 1 的：$P(y=0) + P(y=1)=1$

### 1.3 决策边界（Decision Boundary）

根据上述肿瘤例子，我们提一个问题：$f_{\vec{w},b}(\vec{x})=P(y=1|\vec{x};\vec{w},b)$ 这个式子什么时候为真（=0.7），什么时候为假（=0.3）呢？

看图很容易得到结论：f > 0.5 的时候，也就是 $z=\vec{w}\cdot\vec{x}+b \geqslant 0$ 的时候，结果为真；$z=\vec{w}\cdot\vec{x}+b \leqslant 0$ 的时候，结果为假。

z > 0 就是我们所说的决策边界，我们再看两个例子：

![4.2 线性决策边界](/machinelearning/one/04-2.png =560x)

![4.3 非线性决策边界](/machinelearning/one/04-3.png =560x)

当我们的 z 更加复杂的时候，决策边界的形状也会逐渐变态。例如：$g(z)=g(w_1x_1+w_2x_2+w_3x_1^2+w_4x_1x_2+w_5x_2^2+w_6x_1^3+……+b)$

## 2.逻辑回归的代价函数（Cost Function for Logistic Regression）

### 2.1 代价函数的演变

假设我们的肿瘤预测有这样一个数据集，思考一个问题：我们如何去选择 $\vec{w}=[w_1,w_2,\cdots, w_n]$ 和 $b$ ？

|     | x~1~: Tumor size (cm) | ... | x~n~: Patient's age | y: Malignant? |
| :-: | :-------------------: | :-: | :-----------------: | :-----------: |
| i=1 |          10           |     |         52          |       1       |
| i=2 |           2           |     |         73          |       0       |
| i=3 |           5           |     |         55          |       0       |
| ... |          12           |     |         49          |       1       |
| i=m |          ...          |     |         ...         |      ...      |

在线性回归中，我们用到了 “平方误差代价函数”，它是一个开口向上的抛物线，在英语中一般说这个曲线是凸的（convex），还记得方程吗，它写作：

$$J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x_i})-y_i)^2$$

如果用这个公式带入到逻辑回归中，它最后的结果会是一条不平滑的曲线，会出现很多的极小值点。

![4.4 平方误差代价函数](/machinelearning/one/04-4.png =560x)

那逻辑回归的代价函数到底该怎么写呢？我们先回顾一下，逻辑回归是通过对数函数来模拟二分的概率问题，样本的真实结果 y 要么是 1，要么是 0；而我们逻辑回归公式它本身得到的结果也是 1 或 0，因此，损失函数也可以分成两类：

1. 如果给定样本的真实类别 y=1，则逻辑回归估计出来的结果概率 f(x) 越小，损失函数越大（估计错误）
2. 如果给定样本的真是类别 y=0，则逻辑回归估计出来的结果概率 f(x) 越大，损失函数越大（估计错误）

这两种情况同样可以用对数函数来表示：

$$
L(f_{\vec{w},b}(\vec{x}_i),y_i)=\begin{cases}
-\log(f_{\vec{w},b}(\vec{x}_i)),\quad{\it{if}}\;\; y_i=1 \\
-\log(1-f_{\vec{w},b}(\vec{x}_i)),\quad{\it{if}}\;\; y_i=0
\end{cases}
$$

我这里没有找到合适的图来辅助解释，但是我们可以回忆一下高中数学，对数函数 log 是一条过 (1,0) 点的单调递增曲线，-log(f) 就是根据 x 轴对称，单调递减的曲线，f 就是逻辑回归函数，它的取值范围只有 [0,1]，-log(1-f)就是在区间 [0,1] 上 f=0.5 的位置处与 -log(f) 对称的一条曲线。

因此，逻辑回归的代价函数可以写做：

$$J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}L(f_{\vec{w},b}(\vec{x}_i),y_i)$$

### 2.2 简化代价函数（Simplified Cost Function）

这一步就是把 y 融入到了损失函数中，让公式变成了一行的过程。本来乍一看我想着这一步也没啥大用，但是编程的时候，确实好像会省力一点，减少了分支运算的过程。

$$
L(f_{\vec{w},b}(\vec{x}_i),y_i)=-y_i\cdot\log(f_{\vec{w},b}(\vec{x}_i))-(1-y_i)\cdot\log(1-f_{\vec{w},b}(\vec{x}_i))
$$

当 y=1 的时候，1-y=0 了，后项被消掉了，只剩前项；当 y=0 时，前项被消掉了，只剩后项。简化成一行的公式就做了这么个事儿。因此，展开后的代价函数可以写为：

$$
J(\vec{w},b)=-\frac{1}{m}\sum_{i=1}^{m}[y_i\cdot\log(f_{\vec{w},b}(\vec{x}_i))+(1-y_i)\cdot\log(1-f_{\vec{w},b}(\vec{x}_i))]
$$

### 2.3 实现梯度下降（Gradient Descent Implementation）

梯度下降的过程基本和之前没什么变化，就是重复的一个过程。

![4.5 非线性决策边界](/machinelearning/one/04-5.png =560x)

## 3.正则化减少过拟合（Regularization to Reduce Overfitting）

### 3.1 过拟合问题（The Problem of Overfitting）

我们目前所学的线性回归和逻辑回归，本质上都是要求训练出的模型，要拟合数据集中的数据，也就是让模型和数据集中的数据之间的误差尽可能的小。

如果拟合度不够，会导致偏差较多，表示训练出的模型不能很好的预测数据走向。我们把这种情况叫做：underfit（欠拟合），也叫做 high bias（高偏差）。

如果拟合度非常恰当，也许可以很好的迁移到其他同类型的模型预测当中。我们称其为：Just right（刚刚好），generalization（可泛化）。

如果拟合过度，从图形上看模型虽然很好的通过了各个点位，但是会出现仅适用于当前情况，不好迁移、数据的方差反而变高等问题。这种问题叫做：Overfit（过拟合），high variance（高方差）。

![4.6 线性回归的过拟合例子](/machinelearning/one/04-6.png =560x)

![4.7 分类问题的过拟合例子](/machinelearning/one/04-7.png =560x)

### 3.2 解决过拟合问题（Addressing Overfitting）

**（1）收集更多的数据（Collect more data）**

如果数据量过小，用于训练模型很可能达不到效果。因此扩充数据集是一种方法。

**（2）选择部分特征（Select features）**

与特征工程相反，减少特征的数量，也可以降低过拟合概率。但是缺点是可能会丢掉一些很重要的特征。

**（3）减小参数的大小（Reduce size of parameters）**

之前说过特征缩放，特征缩放是让特征之间的差距变小。如果特征不变，那么参数 w~i~ 就要根据特征 x~i~ 的大小，进行反向的变大或变小操作。这个就是正则化。

### 3.3 代价函数正则化（Cost Function with Regularization）

多项式回归，如果最高次项比较大，模型就会容易出现过拟合。正则化主要是为了减少高阶项对模型的影响，一般原理是在代价函数后面加上一个对参数的约束项，这个约束被我们称为**正则化项**。

在线性回归模型中，通常有两种不同的正则化项，一种是加上所有参数的绝对值之和；一种是加上所有参数的平方和。我们来看下面这幅图。

![4.8 正则化](/machinelearning/one/04-8.png =560x)

这是使用了平方和的正则化项，我们给每个平方都乘了一个 1000，这个 1000 叫做 λ，很重要的参数。所以代价函数的正则化的数学归纳式写一下，就是：

$$
J(\vec{w},b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}_i)-y_i)^2 + \frac{\lambda}{2m}\sum_{j=1}^{n}w^2_j
$$

公式的前项还是平方误差函数主体（Mean squared error），后项则为正则化项（Regularization term），有的地方还会把 b 的正则加进去，但是这里 b 就是一个唯一值，加不加都无所谓。假设我们的多项式回归的式子就是：

$$f_{\vec{w},b}(\vec{x})=w_1x+w_2x^2+w_3x^3+w_4x^4+b$$

1. 如果 λ 过大，那代价函数的取值受正则化影响过大，最后得到的拟合公式就会严重的欠拟合。如下图中的粉线。

![4.9 正则化项lambda过大](/machinelearning/one/04-9.png =560x)

2. 如果 λ 过小，那么正则项对代价函数的影响不大，还是会产生过拟合，如图 4.9 中的蓝线。

3. 只有 lambda 取值适中时，才会有较好的拟合效果，如图 4.9 中的紫线。

那么 λ 该怎么选择呢？吴教授没有讲，打个 tag。

## 4.线性回归的正则化（Regularized Linear Regression）

从代码迭代的逻辑来看，正则化在线性回归中的意义是：让 w 每次乘以一个略比 1 小的数，使它可以收缩，但很小。

![4.10 线性回归的正则化](/machinelearning/one/04-10.png =560x)

## 5.逻辑回归的正则化（Regularized Logistic Regression）

![4.11 逻辑回归的正则化](/machinelearning/one/04-11.png =560x)
